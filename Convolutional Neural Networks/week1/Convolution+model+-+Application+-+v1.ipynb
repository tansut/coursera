{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Application\n",
    "\n",
    "Welcome to Course 4's second assignment! In this notebook, you will:\n",
    "\n",
    "- Implement helper functions that you will use when implementing a TensorFlow model\n",
    "- Implement a fully functioning ConvNet using TensorFlow \n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a ConvNet in TensorFlow for a classification problem \n",
    "\n",
    "We assume here that you are already familiar with TensorFlow. If you are not, please refer the *TensorFlow Tutorial* of the third week of Course 2 (\"*Improving deep neural networks*\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - TensorFlow model\n",
    "\n",
    "In the previous assignment, you built helper functions using numpy to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. \n",
    "\n",
    "As usual, we will start by loading in the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to load the \"SIGNS\" dataset you are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.\n",
    "\n",
    "<img src=\"images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztfWmMZMeR3hd19D19zMnmzJAcksNzxBmKQ4oUKWrEQ6YO\niOs/9Gq9C9ogwD9rQwuvsaRswMAaMEDDwGL9wzBAeOUlsNrVcrWSSMmyZHJI6uA9FO9jOORw7qNn\nprun767uqvSPqnkZka8yO6u6p4rcFx/Q6Hwv82VmvXpZLyIj4gsyxkChUGQPuXZPQKFQtAe6+BWK\njEIXv0KRUejiVygyCl38CkVGoYtfocgodPErFBnFshY/Ed1DRHuJ6CMienilJqVQKM4/qFknHyLK\nA/gQwN0AjgB4FcC3jTHvrdz0FArF+UJhGdfeBOAjY8x+ACCiHwC4F4B38a9ePWQ2b9xYPaBljLyi\nWP5EKNBF6qe1md/aFbhXjXXRyi+n8RsSvOK8O6yehwFWoEtT6+TI0WMYHR2L+gKXs/g3AjjMjo8A\n+ELogs0bN+IXTz5ePXCnR7EaCNUtpg5TH7/+/aDQyo2cR7oLe8L9XsOCFq/kn9MdwHjryNdFaqzQ\nffTck9CtavoBNnWLAGDI18xtyIqmEhiJNwzc08AUw+0q/laBLz5e+vbcENbH1++9L7KvFmz4EdGD\nRLSHiPacGR0938MpFIpILOfNfxTAZna8qXZOwBjzKIBHAWD757bxV5a3Y/d3kLy1gZrAjyl/Wbo/\nuiTe2qFOQm+RQJXv1RzqJzVJ/gHCTX1DicPwDfe3i0bkdxZ4GTe7NxX70m4nuPQpP2cDUmkTAuxy\n3vyvAthKRFuIqAPA7wN4chn9KRSKFqLpN78xZpGI/g2AXwLIA/ieMebdFZuZQqE4r1iO2A9jzM8B\n/HyF5qJQKFqIZS3+lURIp4veCw3u3BtWCu03xLULTsq/KSsUW3dXXY7dnO5H/k18Zyx/O/9eRyOK\npa+PBr5n796Ds9MtrnGtH+yeBvZKwl+a73uRTYP3O9R99L0iT9keeS01daDuvQpFRqGLX6HIKFos\n9psokTJWok537xHx6syjuRGiPWiiujABeyRF3o909/XnmJ5ipHrjuSbdp2tLrG9PbcjaJuV5ezql\np/CGoRHCyo63CyGyu+9L32dz7pXh36fbMuQwxtvZ69KPjnH+Lw198ysUGYUufoUio9DFr1BkFC03\n9Z0z6YXMXCkIC01z+mlY/10e0i7Cfn06aPGJHs9vX2pmr8A1s/ruj2tKlSq5G2iydH+peUScqfbt\nN7eF9fXmvnk5nN8MGOxjJVzKI77b+PWhb36FIrPQxa9QZBQtF/vPCSVp8SbkdeeJxQ/GfzvithC7\nArHywegxj+rgdhF5lFZ9lh43hVDEXyQoZTeqb8YMfmMpUbY+Gopzj2zXnB4XUB1S3YfEck+d8X+3\nQXoG8T6OfZKag775FYqMQhe/QpFRtGG333M+FHPh8fQK009FitSRO91uL5I4JMSUEdNbvbpYWTag\ntgR2mHldLsD0Edy1D+k+YkqxnbhecbwmtNMdqTBF3o/QAxgWtgNen9zLMdiJn4aML5J0AM85K1o8\n9M2vUGQUuvgVioxCF79CkVG0VOc3COnerNysC15k0FawiyaC9UIEDylyieCUmiDwTM2l/kE4VjFk\nbgvMuIn7HdZ33b2HQKRdJKLNhUGENgsa7qHOfs7yzXbNLBp98ysUGYUufoUio2i9qS9CxDGBbCqS\nhq0BzzcGCpmoOOlCcxa84DyCPHL+iJpQl17EDhU7jZCakjJ3RutxgQ8TpSOGuwhxdPj7cM2/TXiE\nhngGg0M3G4CmYr9CoYiELn6FIqPQxa9QZBSt1/krNQ3H1Z0CZJY+PZlCmlSsOcy9LKAk+klBG9B3\nA7ZEEm6wcaQcLgmoj8wjyHkZtDzF2QvTpJS+wRpB/e89aFo9DzCRGwex+QRj9r2W7qOx8/Ww5Juf\niL5HRCNE9A47t5qIniKifbX/Qw2MqVAoPgWIEfv/GsA9zrmHAew2xmwFsLt2rFAoPkNYUuw3xvya\niC5xTt8LYFet/BiA5wA8FDfk0iaJUFSftCi5ffn79hNPBExxqXn4hKombYIpc5DPzBMSqZ0uhVnU\nrzqEiDhC9COxWBFJ3NvJ8j3k0iK6PyJUtlr+JwuTuIQiFGO5/uLQ7IbfBmPM8Vr5BIANy56JQqFo\nKZa922+qP6H+32iiB4loDxHtGR0dW+5wCoVihdDsbv9JIho2xhwnomEAI76GxphHATwKANd97lpj\nfydi+esCgmdKcovbqQ8Kr54dd/dKOadQwEuz4pm9rhLkrwsEDpn6KoDsvV6fvBj3fgh/ykjTQiNB\nP76qFHde/U7j+RMlUl6fK6HfNGElWInQoGbf/E8CuL9Wvh/AE032o1Ao2oQYU9/fAXgRwJVEdISI\nHgDwCIC7iWgfgLtqxwqF4jOEmN3+b3uq7lzhuSgUihai5R5+Fiml3JYDemxIu3F93cRRdPpuf49S\nc2X6f3lRtKvMzyblXGeXnEehI2o8mba5uf0Rcd4lKhV6fZw3ZOiuhXXQ2CtDRi++NxAyh8X13pCO\nLG5CrDdn4DmN9niMN62e23dSAk+FQrEkdPErFBlFa8V+Y1AxVV7ylAlJ0KQ53OVk2wprXsjs4sbT\nOPPwNYwT3IBKaS4pn3ntV6KufPJwUs4PrhN1A9tvScqdqx3fKB/5RkN8F9wcFOKR95OW+AdfAXaT\nFUgvFuw+WBn6diM/Z7Q35EogHIK2XOibX6HIKHTxKxQZhS5+hSKjaCNvv9/Ul9bl61+VVmP9/OrR\nxJ/+EEIx4OiBj5Ly4Zd+K5r1dXQm5cIpGc8wPX42KQ/f+S1R19E/WHcaYfUuzgW5EVdo316B6yqb\nJu2sjzAZRpzhMto6FtkwbLBzNPkVceGNrYy9pxLNuBnrm1+hyCh08SsUGUXrPfzOyVApr7WAaOgh\n8EiLofwgxHXP1QiXzIOP5PcSnDw7npRPjZ0V7UrdvUm5r0dUoXDyRFI+9vKzom7T7V9LynnmGRgU\nr6PDuwKibIpPsX7DVCRcSNb0fJ2pTyLaRcv28qgZlcCdu/E/V/FziTUluqj//Qav8JgcG5m5vvkV\nioxCF79CkVG0IV2XW6gDcsU6TkrBf6/8AS/xCaKaE/8GNl2SlOc6HNl+1gb2gPKiqo9Nf/HjD0Td\nyODqpHzBDbexKck+xGZ8kwQYbrAQh480oqGNel/fQQ7xwHXxsr1z7AmQamjucTvwAWMTlriTDSOo\n8kZC3/wKRUahi1+hyCh08SsUGUUbTH21f5FmonClY4oLeK35osfS3n719V23bf96G5E3vOMm0e7j\nZ35h+6jIiRTyto9eZ69g9O1XknLXWtv/0Jar/HMOs5GycoAQJJaUshE2D99owa+9WV044Anoeybc\nNGdNexDWvzB+z8nfNjylOKLSEPTNr1BkFLr4FYqMon0cfrGyj3sZJ/pYXBB1lcWS7a7YKery/DhI\nIBGX4oqbHK+45Uui3dG9SU5TTB4/IvtgP7cVx4KXX7Bfx6Hnn0rKXavXi3bdA2vqTbw6Z4+kH60d\nOIehgBdhRkv1ESmGNhM10wihhs90G2AwCXEapmHbVsTZeA+/5pQd91Or2K9QKCKhi1+hyCh08SsU\nGUXb3HvT0V1xOlHp7JmkPPra87LdhI20K/QNirrerdfa8sVbkzIVi8H5+qbITZW9Q0Oi3XVf/WZS\nfvFvvyfq5icnkvLk3JyoG1pr+xnosJ/l2J7nRLuLb/t6Us53dgcmXH++QDz9Y5AQJLYPYW5rYLOn\naZde3oXPFJdiMGG9+TcV0jsFy3fVjXb8Dd3G8+HeS0SbiehZInqPiN4lou/Uzq8moqeIaF/t/9BS\nfSkUik8PYsT+RQB/aoy5BsDNAP6YiK4B8DCA3caYrQB2144VCsVnBDG5+o4DOF4rTxLR+wA2ArgX\nwK5as8cAPAfgoSX78wv+/msq5aR85NUXk/LYm6+Jdp15mworVzgp6k58/HFSHrru+qR84Y3STFfo\nsmJ0WmKssDq/nLXxyquT8hV33CPq9jz5D0m5n6Spso+l/TIVO9bUJ++Jdke7++xYN+4SdXnHxGk7\nDB7GIcDE0bS4GjkTX+rxpVG/bUpqrjCzpTs28c+5snkG6s3FX+H3lAymcfegoQ0/IroEwPUAXgaw\nofbDAAAnAGzwXKZQKD6FiF78RNQH4B8B/IkxZoLXmepPcd2fHiJ6kIj2ENGe0bGxek0UCkUbELX4\niaiI6sL/vjHmR7XTJ4louFY/DGCk3rXGmEeNMTuNMTtXD+meoELxacGSOj9V6U/+CsD7xpi/YFVP\nArgfwCO1/0/EDGhJMF1Ti1+XqjD99+QJq8ufPTsj2vV2Wp2ZcvOiLt9h9wNKb+xJynPMPAgAwzu/\nmJR71kq3Wg6hd6aUROu3e+2tXxZVYyPHk/Lx118SdXPz1j15Zs7OvxsyrfeZt+R1HBtv/EpSzjMz\nZtrA1rju2jx/fXMmu/jx/HsPktSVR//Jzy/fgvHz8D22qZTo/i6ckeOiXdPbAUb+j0CMnf9WAH8E\n4G0ieqN27j+guugfJ6IHABwEcF/0qAqFou2I2e3/Lfw/XHeu7HQUCkWr0AYPv+rvSEpSDgRc5fJ2\nmoOXWWKLj99+R7RbZCJPd6c0eXUsWJWgULLlyY/3inbTp2wU3oVf2CXqVrOxESCD5ESRBceD8Ia7\nvpGUnzl2SNRNnbEqTW7SCqIVhxCkp2jvx8hrMlUYddqcAZs+/0VW4Sd/SPHxR5JvrER27ZD65PN8\nC1Duh02axm/Oa5ZfM6T9yXkESGiiR4vI39DAd6K+/QpFRqGLX6HIKNqXrivAKZeWumzbyz5/Y1Ke\nnJBpst56ynLnrZqX3nODvZYvj3L2Ny9Pcie9cta6MBz8zf8VdbNTtm7D1dZLMN/hBAdxwhEHfUOW\niGPbHV8TdS88/te2ixnL/Z/Lya+pwKwJeZJjHXrpmaTcvW44Ka/ZfKl3Tik+ReGRx9SDlOrghyfj\nV52GfrJ7n/rRpJNg8BLxOV1O/Gjywug74p9L0xaVxuV+ffMrFBmFLn6FIqPQxa9QZBQt1fmNsdFH\nBFcvzol2PuQLVr/esesuUVdhJsGXnviJqJuas558Q73Wk66/V5Jh9PbZ1NimIj0I9//6l0l57OiB\npLz5BhkZ2Lt6XVKmnGTppJz93JuuuFbUbdy2Mykf2POCvabimIbKto+ebmnSLM3bz/nhb+18b/jn\n94t2HV0sZ0BK1+b57eBHnDOaN/dfauhYno/YEMIAQnp8uAdnP8B7g5q1g4ZIbZZPHMKhb36FIqPQ\nxa9QZBQtNvWZhKTCpH53KryVc5UHjsh13RdZWmtH7Hr5p08m5enTNrR41fSUaNc/Z9WAYkGK7N19\ntm7y4L6kvPfUMdFu/bVWfF97+TZRV+yxRBz5nDQRbvuS9ZY+9L71XhwbPSXaFXL2jhQ65FfY3WPF\n+ckj+5PykXck8cmWG25lR34O+2al12gzXazusMRovmt8XaRSXMf2H9JbmkwBnub4T3cdmFHT0De/\nQpFR6OJXKDIKXfwKRUbRWp3fAJVzZipHdxKc+CkyBU7WwEk0/W60227+gjjuG+hPys//zOr/Rw4d\nFO0KE5NJeZVjBhxmZrtCwd66wpw0CZ7Y82xSHj3wgahb/7mbk/LqzZeLuv411vX36ltuT8p7nvx7\n0W6mxEyVpizq+K3rYXsWh19+RjQbHN6clIcuvBg+SL2zOeJMX/7AhroUz0DUsNXhIr1vRVWAqJRM\nyMU5QCoiyiH3Xs8eQrqhU9c4mYe++RWKjEIXv0KRUbTWww8GFa+ozlIdO204h70R4qQj8sIvMm25\n2nLprxm20W6vPS9Tfr3ytE2NXZqVPIA0Zr3nFss2anBoVY9o19VpTXizI9IM+OHTP0rKQ5d/Ts7x\nRsv3d8UNVm059sGbot30cUsCMjEtVQ5+5zihCc1Jk+ZHv7URi9d981+Kus6eVVgufNJns1J/dB+h\n7F8BApPwpAJp2z3dhE2HAZ0XkeZCR/2o1Bo3cj/1za9QZBS6+BWKjKL1HH7n+OhcAgnyHngJH1z6\nZYeFQtaw8VYNDiTl2++RhBrDmzcl5V/9nydF3ZlTlmNvkakic4wfEABWseCg3q4uSFhV5fCe34ia\nMcbpd+WX7Ly23/VN0e6lHz6WlGdLUjXhYn+pbMfqdLwV5w9YD8X9Lz0r6q643Y6dL/BHJCRwx7kC\nNkv/HT1SKkjJ118DhB1Ut1iv06g+YuOXgt6K7hJp4r7qm1+hyCh08SsUGYUufoUio2gDb399T6Rw\njBVXmOzvVdgByjEX8r0Cpq+7ut4V22wU3hDzuAOAX/38p0n50AfvJuWZuZJo1zNpzW/9vdIM2NNl\nCUN7OyR5aInlDPjdE1avv3D7F0W7i7ZbEtOPXvyVqFus2D2ABUb6MV+QX3Vn0e4BHHL2Hjp6Lff/\nJZ+3RCX5opyvtFAF9nAi9dHY1NsNRQZGdpIiMfW2jdsbCPURNFU2ea+a2UpZ8s1PRF1E9AoRvUlE\n7xLRn9fOryaip4hoX+2/ZuFUKD5DiBH75wHcYYzZDmAHgHuI6GYADwPYbYzZCmB37VihUHxGEJOr\nzwA45x5WrP0ZAPcC2FU7/xiA5wA8tHR/Sclzvg5XGbHfqJBJkJn+XDIPw1JemQBxHFcl1l5wgaj7\n2r/4g6T86q+teex3v/q1aDc5ZoODpqZmRd36Ies9t+qCtaKuu9MOPsOChQ69+pxot9C9OimX8pIQ\npLIwZ8tl+9nmczKPQaXbBi31FqSn5IEXLPefKVsz5pab7hDteBq1FFZE1PcZ+PzPjgvxFAQjjOJm\nEey/adRPI5YOcAt00YQNNWrDj4jytQy9IwCeMsa8DGCDMeZcvukTADY0PLpCoWgboha/MaZsjNkB\nYBOAm4hom1Nv4PmBJKIHiWgPEe0ZY77xCoWivWjI1GeMGQfwLIB7AJwkomEAqP0f8VzzqDFmpzFm\n59DQ4HLnq1AoVghL6vxEtA7AgjFmnIi6AdwN4L8CeBLA/QAeqf1/Yqm+jGHppkNBVSkbHovG4nXk\n/naRpwyhzIv9gFQEIR9L1nUxPfm2u76alC/YuEm0e+ZHNnJv7oz8TeRkJPPzc6Iun7OmtGKem9Wk\nTj43bvscOzUq6rZsXJ+U1622psrFRemCzD9Zh5NGHCxi8QAjAekekprd8JXXJeUwD36QwRO+yqaI\nREImsIBpUh7GJyuIpQHlBB6uVzr5bo9xeydfVVOmvhg7/zCAx4goj6qk8Lgx5mdE9CKAx4noAQAH\nAdzXxPgKhaJNiNntfwvA9XXOnwFwZ/oKhULxWUDrPfy88kmAo43JRYZ7+AU8AVNplDyRWen52BNp\n4hHmGcjSfF9+jUy71cUi+V7+uYwMnB89kZTHJiZE3dSMFb9zXE1x5sGGxkUb5D5KJ+P0v2C19dTL\nO2m+Rycsucfsguy/WLQkIHMz1my599e/EO3611+YlHuHpNkyFiFTnzc9VQMyLnm4/9xnR5qa5f0Q\nbZ3nSpoSfRXyRDACL8RVGGvSjIT69isUGYUufoUio2h9uq5EhHUpkAORIHx7VOzAOyIYBcR+Rrud\nY6qDK+IJETvFjuzxEnTabdxyaVK++w/+SNS9+4INxPnkjd+JutLE2aRcZkE5qMgBLt5kd/Ev2iy9\nEE+cOJ2Ux8etX0V3hyQV6WFpviYnz4o6nu04R7ZcciwXh15/MSlftesboo6rRc2GoJDnmQiSYUTu\ngrvqBh8rzO/nep9yKxKzFKXm4WcEIY++0BjfYeOCv775FYqMQhe/QpFR6OJXKDKKNpj6yrX/zu+O\nUO8CnlJcx0rp9bxP2b/YD+CeUjnX5MPzBzj9V/hxmbVzTYK2rm9wtajZ+VVLxnnBJZeJutd+abn0\nJ0dsWu41Q32iXY7N48ypMVHHtwrGGanIbEFG9XUyYo4Oh+hjfNKaAfv67NjFvLwfIx++lZQ3btsp\n6vrX29wIUnNvjrlfZLFKVfqvE9+14O2PRzNmtGiSziWvZH1ErZH4kfTNr1BkFLr4FYqMorXpuoxB\nJeGSd1JthcR50Yx5+OUc0V5YAV3efm4G9PMAEjMJOlZAId4L9aAsg2Zmx88k5dKUDGNenGfkHvMy\nhdbmLTZAqPMCm1V4qF+K/SPHrcnt2MhpUVdg3oUzLN1YR0Hy71HF3v9169eLOv6xzzIVYGiwX7Qr\nz9q6T34n055tu/v3knIuJ3MGcATJPDxROU1S/y+BQPBR4Hnk85c8M6GcEgGOSt815wH65lcoMgpd\n/ApFRqGLX6HIKFpr6jMm0Y/TUXf+NMhCfxK6vGxZYXWp/QBB4MH1R3e0+vz+1WOr25cYweb+lyV3\n/tzx/Um5UJGc/osL1uRGTqTdUI/l+F9k0Xmr+iX3/2LJsqQfPn5G1I2O2D2GCvucq/p6Rbsyy+NH\nHZ2irsT2MPp6LIEJOYSdxPZtju99S9Rt3m5TjA9dIMlOvEi5xHI9OWCmC+jkK6I2B8kx+V4VO5vK\nFVnvijpDRY0Eb3RhI2ZJffMrFBmFLn6FIqNouYdfpSY6u2I5T8OVzrzNzHRMHHYo9qSDnxt8hTyr\nqtQtV0+wOkfsJ2PF3PGTR5PysfffEO3W9Vtz26oeKVKfHbe8fXOz0kTY3285/cdOTyfl7nFJ+tHR\nYUk/enu7Rd2pCXvd1CxP3SVvSEenNf2ddvrv67d9DgxaM+PCvD8d+Lxjtjy69+2kPMhIP9LRnLG8\n/f6zsempBbFHiJQjahbnrvMweARc/GK9/9LnIzxfG5D79c2vUGQUuvgVioyitR5+MNYjKhUMw+Hn\n3wvRL4cENCle1t9Frh7yY2eOTFScGrcBNZWKFN8LefubWnRosRcWbf8lh6RjvmQtATzN16o+SfHN\n1YWSQ8m9dq21BMwcs56AU3NSZO9hAU1zEzLop3+A7fCL7MbSKzPPPlvOCZA6/pHNYrz1pi8n5Y5O\nqaYExW3j++JTLeN69BmN3LrASKkufZaG1CMcojbn/QXGCon95ytdl0Kh+KcHXfwKRUahi1+hyCha\nT+aRKtTAvfNCgV48Oi9FsMm7czupv8eQiioz/v0A0Rvz1CvmZdRagZFj5F2vOBbhNuOk6/rk0Mmk\nXJqxJrvZaenhd3qMkXTMS3191YA1F/YPWK++OafdAmP9yDn3Jsd07dIM81DMO16TOf/extSo/SyT\nZywxydoLLxLtjMfzEkjRs7JrmnPb41flGlPsPXNqcnC/c6uTIyAwpSZJUTii3/y1NN2vE9HPaser\niegpItpX+z+0VB8KheLTg0bE/u8AeJ8dPwxgtzFmK4DdtWOFQvEZQZTYT0SbAHwDwH8B8O9qp+8F\nsKtWfgzAcwAeWrKzmonPBH53UllMRSVv5xfZK44ImTMeU18qMyzn7ffXlRcZUUZR3sYiP3ZsN8WC\nFfunpqX57cARKx5vXGM962YcT8CpWasuuCIwz8Z74QbL73/qjOTmn5i2qkNXXorsA93WQ7HAVIJ8\nQXL/c0WCHNWnMmfVltFjB5PyGkfsl/CLsiZA4hfK5usznTXEsRdpSYxFsItIk2NDwU0exL75/xLA\nn0EqzhuMMcdr5RMANqSuUigUn1osufiJ6JsARowxr/namOrPct0dByJ6kIj2ENGecceHXKFQtA8x\nb/5bAXyLiA4A+AGAO4jobwCcJKJhAKj9H6l3sTHmUWPMTmPMzkGHA06hULQPS+r8xpjvAvguABDR\nLgD/3hjzh0T03wDcD+CR2v8nlhzNWN2NUvp6pPsjj8gL5GUjl4jD4yOcIl0QJJ2yjwpzb12Ys+63\nedcExghCXS/mAjP9LS7Kypk523+x05r3Fiqy/wr7LG7OAE440tVhTX0dRamT8/u/qlfq8uuGrLlQ\nfE1FSQI6ze5HqST3Jfi8Th39JClffsOtol2OQtGcfMKe8w5S7rY+c22Tunvz3CCcjCR28DhXZaf7\naCzHyecRAHcT0T4Ad9WOFQrFZwQNOfkYY55DdVcfxpgzAO5c+SkpFIpW4FPj4cfF0GBKJ16suJ34\nzXS+dM8UMPW5UWyc926REVtUylL0Li+w6xwzWo55xbmRcH29Vqzu7LSi+MSM9AScZdF/cFJodTEe\nwDlmEiw41Pl86HUDA6KuwPoss/tYhhTtOXmKMW4eBnvdmWNHknJpdkY06+yROQl88KXdSh2nSFw8\nCHqRun3EydTBqLvgdXGgoBlQU3QrFIpI6OJXKDKKlov9PvEk5KUVK9LInXq/KUD25uyWsz4qIbGf\nifrzTtDM3LwNhnEz4HLCjvHJaVG3wMY+Pmo98ubnXO48O4+NF8pUW5wAY3HRzqPgWFf6WVqvdQ41\n+Cy7bm6R7egvOqI90x3KTsoyYvdq4owN8hk9eUy0G75kK7vIpVu3CAXzcHE7Wvh1jQKBpiH+mGj+\nPZ+bKqSay5ulnfYCn7NxCj998ysUWYUufoUio9DFr1BkFG3Q+etDmHJCaZucGnEU4EYXKboCewN8\nrIpjSqwwr8EKG2x6xiHHZB5/HQ6ZBzcfdjuedUcnbT8TpyxBqClLXXsNI+koFOTv98SE3Svo6rB1\nZcccObx2MCnnClJTLLHxuD6ac5TQMvMmXFyU+x48bfk8Mzm+/ZunRbv+oXVJubd/UNT5UrV7STOr\ntc6R59lxH7LIqLj0bhTXw/nGUnxegNjB4sg940fSN79CkVHo4lcoMoqWi/0+fj5JzOG3w5gAn1qQ\nG12oBNxbzG3I+3N+G8m6yVGHNY+NnZVea8Qz8TriZaFo+9w4IDPnnpqyJrbTYzb8Oe948fV2W6/B\ns6Ojom6Bmek6O63nXtlRHYp55rlnpJmul3Hr8/s2V5Kqw/QsO3b65x6EOfbdHv/obdHu7RcsDcSO\n2/+ZqOPeipJHz5/yK+iN1wS3vTterLdf0/2vMHFICPrmVygyCl38CkVGoYtfocgo2hDV51P6WTng\nehnMSeYP7nJCtUK/ecwkSK7Ob48Hhi0R5WxFTnh6zurdax2X2J4Om7K7a0Cm7761yx4vlK0JzL1n\nM8zdd3poznYtAAASo0lEQVRWRvzli7aPoX5G5rFGRs/191i9vqvDyS1A9U1Wxtl7WGDH8wG31yIL\n/yuzPQkA2P/680m5s3+1qLtqx86k3M31f/jdgEN6ckhb589VY5z4pm6rVLSoj5sfcBJOxLkqp+vO\n7b/E70nom1+hyCh08SsUGUXrPfwipJI0WUP9o1RX/sxPjkoQ8iYMmAGZ2L/6Aiv2X3z5laLZZtho\nvcFuKdrPz1g1YGZBesUVmSmx2MlSYTli+WCf7bM0L/svsLadrJxzeAYXmToyP++k3uZtBR+hP09C\n2eEjXCybes1SxCdzE+NJ+ZX/9xNRNz56Jilff8uXk3L/oPQEzLEUaEGSi1DkKFdvXO5//1VOu+Z0\nDiNS1cXNwz1yiWdioG9+hSKj0MWvUGQU7dvtb8ARSyTXYjx9rgdegC9BJt8V553gHS52uXWcDpyJ\nWVcPrxXtOidsHwsVSZ63ULR9jk6fkXVzLIVWD/tqHAI+w4Jm3Cy9YMczLOiHHLGfBze5gTIFlnqL\ni9QV56bOzdt5zMxLL8E5xmNY5ipAijPRlicmToi615/5aVKeOm3rtu74gmi36dLLk3Jv3ypRxzMJ\nEwVUOn5NJIdk+kI/D6VQHSJVgNBYLmX7wkzVI9QloAlB3/wKRUahi1+hyCh08SsUGUVLdX4DA3NO\nb3b0TJ9O7tYF3blEn27kFytz1d0ZjB+7ZB6cEGP+9PGkXDp1XLSbZWa0CknCjqkF1gfJ9Fdnp04n\n5V5jv5o5N/WY4WnDpM5fYim6ucnOTSmW4956KZYIZt7j98O5VyVmqhyblp6G3PLH8xOkCEFYNGDF\nMReSsSbTg2++kJSPfPS+aHfRNTuS8hXbbxJ1Gy/ekpS7u62XIOVCz06AQDZlzYsMwwuwgHLSVb6v\nknOfYW5anZfkr9NnqySplbKzBxRA1OKvJemcBFAGsGiM2UlEqwH8PYBLABwAcJ8xZszXh0Kh+HSh\nEbH/K8aYHcaYcw7XDwPYbYzZCmB37VihUHxGsByx/14Au2rlx1DN4fdQ8ApjxTw3JZIRmbZc85vH\nVBTwzguJ/Vycd0XZMjux6Hijca+46TFLojE5Kk12lQXWf06K9pNMKptekNx/M1NWlCvBqgvFBWlG\n4/MqlRyxn4niRZYzoKPDMRdysdf5Mvg9WFhk3meOqLzIRPapORmww6XojiLzNHT6cOfPwVObgZk3\nS+MyG/zHrzyXlI98+K6ou3DrtUn5yu02UGjTRVtEu+5eGwSVc/MHREbYCFKRgLk6nVKMe0PyVGlO\nlmh2D6bH5TM3eqqqMi46z0oIsW9+A+BpInqNiB6sndtgjDmn7J4AsKH+pQqF4tOI2Df/bcaYo0S0\nHsBTRPQBrzTGGKL6BF21H4sHAWD9urX1migUijYg6s1vjDla+z8C4McAbgJwkoiGAaD2f8Rz7aPG\nmJ3GmJ2DA/0rM2uFQrFsLPnmJ6JeADljzGSt/FUA/xnAkwDuB/BI7f8TS/VljEn0RNfFsczcEt3o\nMW5iq4hceo5OFDAX+kx4KXMeO3a57rmuPUuWRHNkQerT06etPrZYkbd4osRdcyXxZ57x4E+zz9bZ\nKfvnc553IgO5m22xaOeYd/j9y/xeueZOVinIQ3OyjxLTL8vOfexhewx59lnmStL9dJ71UXD2A7oK\nbL+ETbIj75oLWW7B00dF3f7xU0n5xMdWYN1w2dWi3VVsP+DiLZeJOp4uPW1djsvWF4pTDbmlcyyU\n7OecnJoQdR/tPwAAmJ+Xey8hxIj9GwD8uPYhCwD+1hjzCyJ6FcDjRPQAgIMA7oseVaFQtB1LLn5j\nzH4A2+ucPwPgzvMxKYVCcf7R+qi+mnhYdkR2Lva74jb3AhMeYSmx32/C46JyWYj9sl05pBJw+bho\nRcGBbbfIdkcPJeWJcSmegZnzMCO9tOZK1ktujnHp04xjvuEqTF6m186tsiarhYVZW3Y8wvgnczNv\n8+9GRLiRvFlck+hzVJMcq1xg93HOMUWJHo1fBVtk5VTMHY9QdMVmZk6dP30kKR8+e0o0O33wQ1t3\nzfWibvtNtyblodVy01qYIwO5BeR8U2ei2vHHccp5JvbtPwgAmCtJ83EI6tuvUGQUuvgVioxCF79C\nkVG0IUV3VXEJBke5abNFeu365wHHRJjqY+n+UhNxzTqsirup9jp6YPfQmqR8YWpPwZ5YcN12hemM\nmT7LcpKcnaa7R/Lxd7LItZkJa3L88PXfiHajJ63+m2IDYuxD3N3UjcjL55hO7tysWeYWzN1S3f2F\nBbaHU0iltbbz6mUuwh1F+c4qsC8m75gjuxmJaZExIpUr8jMvnDqclN/7jXRZOXbgo6S87cYvirqL\nOYtQr/VjyTnzgHAZjtsPcJ/vBWbWnZ13Ij2TCNH497m++RWKjEIXv0KRUbQ+RXdN4nGju3KGEUg4\npo886pt5yo5dh4cXVByRiVup+NBl5+dPkor4CUdkpKHbzk+UIfpwVAJuWhSzJ8eMlrdfWz7n1LHj\nnh5r9lv1ld8T7Q7ufSMpH9onyTFmZ6znIZ+im+Z7dta2m11w5HkZpslOyw/dXbTzXdVTlHXMXNjB\nSEUdXhJ0MXG+KG+HUCXEI+dGlfIHZHFW1I3tfy8pPz8iiVs+ZGQhl19pvQY3DF8o58jUsUJeRnqK\nnAHiXslneGrammtHTx8TdeUaiYurKoSgb36FIqPQxa9QZBStFfvJiqWOxIsCk8PcWAl+WGG7pnkj\nf7sqIUsA50kLeALydsYV2Tn3n+ea6jErO58lxBFY8Xgeuv1X2B1ZTLky2h1h7mWWL3aLZldut7vW\nW7dJ3ruKyKTLOAfnpJfge2+9lpQ/+uA9UTfHVAJ+3/LO+6az04rAA72S77Cvy6oBfIO/kLLCmLpl\nQIr6ghvE1VKEuiefzg7Gi7gwIUk0Dr9jmesO7bOBQ6sGZcbhwTXWAuTmFiiyzM2cd7/iWHkmJqy3\n6LFjUuw/c6o6r9KcVFlC0De/QpFR6OJXKDIKXfwKRUbRUp2fQGnPpxoErbmj03FCRR7B5eaOg+A/\nd6o8ZrqQzp/m9K9PtJiO/mP7Bm4nwoPQ0flTs64/D+7955KMyqhHbiJ1vPOYvYyTfgBAZ0c3q7OP\nSHfvgGh3021DSXn9BRtF3euv/DYpjzGCUzdic2za7i+4Ho9r+7pY2erFnT3SVMate64XongS2EaK\na2ZllsSUmTif49+13CzoZMel6bNJ+ey0jOYcO2YjPStujkmqvyZSZDJs76G8KO9Vrva9k+bqUygU\nS0EXv0KRUbQhsKcK91fHcNE+lUrJij+GXZlzJWp2WUqc95B5uJE9XBx0BUjyqA4pwV6oFX6eQZfQ\nxHhMfaG0YYtOpAwPDuIqgUt8wtUAVxUrcI85phJ0OO5znN9v/cZLRd3OW62Y/u7rLyflk8cOi3YV\nNsfpBfk5FyYsuckUCz4amu0U7Qa6rRrQ2ykf6WKOe8+xz5x3PSN5ajMnrTq7p5Tzq1kFkQbOVemY\nOuY8E75UdRVHLSzxPpx5nPs63aUTgr75FYqMQhe/QpFR6OJXKDKKFuv8JonYa0A18bZ1XWedoeSh\n0MP9JJ2xefxCJKA+s6Lb1tXDfX2m52gr02SnvK6+2Q+QOn9qjjwVtMgLKN8VIu23o8cWugaT8mXX\nWk58ykuz4qmjB1gXcv+Cm3L5fkBlRnLTz7GIwv4u2f+qTnvcWeSRgQ5xCNtXcs2ixQ6W/8CZY5mF\nhfI8Bq5Ldijfn88tveL00ckiNk3FdW2vXqc6v0KhWBK6+BWKjKJtHH5pxzdmkgkkN+J1Lm8BF5nc\n9FG+FF0hsd8JqpJkG/W5KqrtAtGFIsN4k6qJSF/mispMJeCmw1SUIydIcURI34cLzcN1UjMsnVln\nj/UE3HTZ50Q7bnIbP3FQ1BFLX8bFWdcPcr7CzYXS843zHfKaTid9GTfuOdnABC+gYwUU6GLp2Bsh\n1eAQ6egclS7HciO4dee8Pl2SnBCi3vxENEhEPySiD4jofSK6hYhWE9FTRLSv9n9o6Z4UCsWnBbFi\n/38H8AtjzFWopu56H8DDAHYbY7YC2F07VigUnxHEZOkdAHA7gH8FAMaYEoASEd0LYFet2WMAngPw\nUKgvg7ToKCqTokvJzURZvpOOgGgf2mU3AXE4QOsdS/8tjlNSf+TYQjXxi/Y+8a9a5iqAnIdILeWk\n4TJU3yuOXM5EJg7nnB3yHHuvFFhqs1WD60S7TZdtr9sOAMaPf2wPWCZeR2IXO+mLzv2eZ/cnx6wT\nlQV/6rFOhyRQfO/O81tkfIq8IbmZeJk4nrpXQlSv/6y7cNXac9mO/VmD04h5828BcArA/yai14no\nf9VSdW8wxpxjMzyBajZfhULxGUHM4i8A+DyA/2mMuR7ANBwR31RfWXV/p4joQSLaQ0R7zp6dXO58\nFQrFCiFm8R8BcMQYcy4644eo/hicJKJhAKj9H6l3sTHmUWPMTmPMzoGBVfWaKBSKNmBJnd8Yc4KI\nDhPRlcaYvQDuBPBe7e9+AI/U/j+x5GhCPnBJL/0KTrR3XsDE5qsLWOJSnl5CjxPpmCX4L2pK1WZd\nuGaZSqW+fp2eh7hKDuAJETOpSDJ/VJ/s3XaSc98Vhu8pOB5nnDiT3atCQRJx9PVbostNl0ozYFe3\nTUU2csiSY5bmpAQp9gBSjxHbA2H3prsoH31OqLGQch3l88/7qoSJMOfsj/Dv2k0jztV0YkbHkPpe\ncqI5u2qNGzH1xdr5/y2A7xNRB4D9AP41qs/440T0AICDAO6LHlWhULQdUYvfGPMGgJ11qu5c2eko\nFIpWoeUefn4KDN4mwNIRaaYLUeeJnt3Ms0IG85t8OJdCbFovQJpoyDHTcZNbjjGVELm8bNwb0kV9\nE1657CgLwizl60Ei7a3IORMdMdfjkeeKsvmCfQR7+vpF3QUXX5WUu1jdsf3viHaz46eS8qKjBnFR\nnw9eyMv55nluAUey57kiyK0UBDK2z4LjJhgKguIqGZFfxRD8hHmZZbiyWEldvxTUt1+hyCh08SsU\nGYUufoUio2h9VJ/XpMf1+tRFnrK/67TpsL5pjlxSB/Z7SI4JzIg8gXwsV5/mZUcX5u6mjlstP+am\nIpfXnR+nchxwYk62V7BAMtqtErilcg9DGj9lQ7b/4uY19F7l6qTsczr3u7PT5g9Ys+Eie76rT7Q7\n+OHrSXli5IioE1GOggjWvR/+/aIuYZ6Vc+xgz0+BkW3kHB9kaYJzvk9eww6Krs7PyUjdHJB0ftx7\nFQrFP0Ho4lcoMgpqlnSgqcGITqHqELQWwOmWDeyHzkNC5yHxaZhHo3O42BizbulmLV78yaBEe4wx\n9ZyGdB46D51Hi+agYr9CkVHo4lcoMop2Lf5H2zSuC52HhM5D4tMwj/M2h7bo/AqFov1QsV+hyCha\nuviJ6B4i2ktEHxFRy9h+ieh7RDRCRO+wcy2nHieizUT0LBG9R0TvEtF32jEXIuoioleI6M3aPP68\nHfNg88nX+CF/1q55ENEBInqbiN4goj1tnEfLaPJbtviJKA/gfwD4GoBrAHybiK5p0fB/DeAe51w7\nqMcXAfypMeYaADcD+OPaPWj1XOYB3GGM2Q5gB4B7iOjmNszjHL6DKh38ObRrHl8xxuxgprV2zKN1\nNPnGmJb8AbgFwC/Z8XcBfLeF418C4B12vBfAcK08DGBvq+bC5vAEgLvbORcAPQB+B+AL7ZgHgE21\nB/oOAD9r13cD4ACAtc65ls4DwACAT1Dbizvf82il2L8RwGF2fKR2rl1oK/U4EV0C4HoAL7djLjVR\n+w1UiVefMlWC1nbck78E8GeQnB/tmIcB8DQRvUZED7ZpHi2lydcNP4Spx88HiKgPwD8C+BNjzEQ7\n5mKMKRtjdqD65r2JiLa1eh5E9E0AI8aY1wLzbNV3c1vtfnwNVXXs9jbMY1k0+Y2ilYv/KIDN7HhT\n7Vy7EEU9vtIgoiKqC//7xpgftXMuAGCMGQfwLKp7Iq2ex60AvkVEBwD8AMAdRPQ3bZgHjDFHa/9H\nAPwYwE1tmMeyaPIbRSsX/6sAthLRlhoL8O8DeLKF47t4ElXKcSCWenyZoGqw9V8BeN8Y8xftmgsR\nrSOiwVq5G9V9hw9aPQ9jzHeNMZuMMZeg+jw8Y4z5w1bPg4h6iWjVuTKArwJ4p9XzMMacAHCYiK6s\nnTpHk39+5nG+N1KcjYuvA/gQwMcA/mMLx/07AMcBLKD66/oAgDWobjTtA/A0gNUtmMdtqIpsbwF4\no/b39VbPBcB1AF6vzeMdAP+pdr7l94TNaRfshl+r78elAN6s/b177tls0zOyA8Ce2nfzEwBD52se\n6uGnUGQUuuGnUGQUuvgVioxCF79CkVHo4lcoMgpd/ApFRqGLX6HIKHTxKxQZhS5+hSKj+P9b7K4+\nyuoTDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce7a1047b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 6\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Course 2, you had built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 - Create placeholders\n",
    "\n",
    "TensorFlow requires that you create placeholders for the input data that will be fed into the model when running the session.\n",
    "\n",
    "**Exercise**: Implement the function below to create placeholders for the input image X and the output Y. You should not define the number of training examples for the moment. To do so, you could use \"None\" as the batch size, it will give you the flexibility to choose it later. Hence X should be of dimension **[None, n_H0, n_W0, n_C0]** and Y should be of dimension **[None, n_y]**.  [Hint](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    X = tf.placeholder( tf.float32, name=\"X\", shape=(None, n_H0, n_W0, n_C0))\n",
    "    Y = tf.placeholder( tf.float32, name=\"Y\", shape=(None, n_y))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(64, 64, 3, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Initialize parameters\n",
    "\n",
    "You will initialize weights/filters $W1$ and $W2$ using `tf.contrib.layers.xavier_initializer(seed = 0)`. You don't need to worry about bias variables as you will soon see that TensorFlow functions take care of the bias. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically. We will talk more about that later in this assignment.\n",
    "\n",
    "**Exercise:** Implement initialize_parameters(). The dimensions for each group of filters are provided below. Reminder - to initialize a parameter $W$ of shape [1,2,3,4] in Tensorflow, use:\n",
    "```python\n",
    "W = tf.get_variable(\"W\", [1,2,3,4], initializer = ...)\n",
    "```\n",
    "[More Info](https://www.tensorflow.org/api_docs/python/tf/get_variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 2 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [4, 4, 3, 8], initializer = tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [2, 2, 8, 16], initializer = tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      " -0.06847463  0.05245192]\n",
      "W2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output:**\n",
    "\n",
    "<table> \n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W1 = \n",
    "        </td>\n",
    "        <td>\n",
    "[ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394 <br>\n",
    " -0.06847463  0.05245192]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W2 = \n",
    "        </td>\n",
    "        <td>\n",
    "[-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058 <br>\n",
    " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228 <br>\n",
    " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Forward propagation\n",
    "\n",
    "In TensorFlow, there are built-in functions that carry out the convolution steps for you.\n",
    "\n",
    "- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W1$, this function convolves $W1$'s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "\n",
    "- **tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME'):** given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
    "\n",
    "- **tf.nn.relu(Z1):** computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "\n",
    "- **tf.contrib.layers.flatten(P)**: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)\n",
    "\n",
    "- **tf.contrib.layers.fully_connected(F, num_outputs):** given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)\n",
    "\n",
    "In the last function above (`tf.contrib.layers.fully_connected`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. \n",
    "\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "Implement the `forward_propagation` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. You should use the functions above. \n",
    "\n",
    "In detail, we will use the following parameters for all the steps:\n",
    "     - Conv2D: stride 1, padding is \"SAME\"\n",
    "     - ReLU\n",
    "     - Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is \"SAME\"\n",
    "     - Conv2D: stride 1, padding is \"SAME\"\n",
    "     - ReLU\n",
    "     - Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is \"SAME\"\n",
    "     - Flatten the previous output.\n",
    "     - FULLYCONNECTED (FC) layer: Apply a fully connected layer without an non-linear activation function. Do not call the softmax here. This will result in 6 neurons in the output layer, which then get passed later to a softmax. In TensorFlow, the softmax and cost function are lumped together into a single function, which you'll call in a different function when computing the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1],padding='SAME')\n",
    "    print(Z1)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 =  tf.nn.conv2d(P1, W2, strides=[1,1,1,1],padding='SAME')\n",
    "    # RELU\n",
    "    print(Z2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2,ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    # FLATTEN\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    print(P2)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, 6, activation_fn=None)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(?, 64, 64, 8), dtype=float32)\n",
      "Tensor(\"Conv2D_1:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      "Tensor(\"Flatten/Reshape:0\", shape=(?, 64), dtype=float32)\n",
      "Z3 = [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064]\n",
      " [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "    print(\"Z3 = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <td> \n",
    "    Z3 =\n",
    "    </td>\n",
    "    <td>\n",
    "    [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064] <br>\n",
    " [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Compute cost\n",
    "\n",
    "Implement the compute cost function below. You might find these two functions helpful: \n",
    "\n",
    "- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  [here.](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "- **tf.reduce_mean:** computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n",
    "\n",
    "** Exercise**: Compute the cost below using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return tf.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n",
    "    print(\"cost = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "    <td> \n",
    "    cost =\n",
    "    </td> \n",
    "    \n",
    "    <td> \n",
    "    2.91034\n",
    "    </td> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Model \n",
    "\n",
    "Finally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset. \n",
    "\n",
    "You have implemented `random_mini_batches()` in the Optimization programming assignment of course 2. Remember that this function returns a list of mini-batches. \n",
    "\n",
    "**Exercise**: Complete the function below. \n",
    "\n",
    "The model below should:\n",
    "\n",
    "- create placeholders\n",
    "- initialize parameters\n",
    "- forward propagate\n",
    "- compute the cost\n",
    "- create an optimizer\n",
    "\n",
    "Finally you will create a session and run a for loop  for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function. [Hint for initializing the variables](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = False):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                print(temp_cost)\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model for 100 epochs. Check if your cost after epoch 0 and 5 matches our output. If not, stop the cell and go back to your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.79953\n",
      "1.89293\n",
      "1.78433\n",
      "1.79885\n",
      "1.83339\n",
      "1.82178\n",
      "1.81995\n",
      "1.81698\n",
      "1.79159\n",
      "1.79067\n",
      "1.78114\n",
      "1.79204\n",
      "1.7821\n",
      "1.7922\n",
      "1.79801\n",
      "1.80788\n",
      "1.7835\n",
      "1.79582\n",
      "1.79005\n",
      "1.78858\n",
      "1.79734\n",
      "1.78544\n",
      "1.78231\n",
      "1.79968\n",
      "1.78692\n",
      "1.80558\n",
      "1.79832\n",
      "1.78697\n",
      "1.78332\n",
      "1.78118\n",
      "1.79007\n",
      "1.78717\n",
      "1.78804\n",
      "1.78681\n",
      "1.78412\n",
      "1.77962\n",
      "1.78652\n",
      "1.79052\n",
      "1.77945\n",
      "1.78369\n",
      "1.78065\n",
      "1.79057\n",
      "1.77443\n",
      "1.78777\n",
      "1.77585\n",
      "1.77713\n",
      "1.77756\n",
      "1.78625\n",
      "1.77335\n",
      "1.77722\n",
      "1.78555\n",
      "1.76392\n",
      "1.74685\n",
      "1.75075\n",
      "1.78468\n",
      "1.76531\n",
      "1.75872\n",
      "1.74791\n",
      "1.76207\n",
      "1.75561\n",
      "1.75242\n",
      "1.73285\n",
      "1.72943\n",
      "1.71652\n",
      "1.72672\n",
      "1.69653\n",
      "1.69846\n",
      "1.72444\n",
      "1.66447\n",
      "1.69561\n",
      "1.71112\n",
      "1.62238\n",
      "1.68228\n",
      "1.68177\n",
      "1.67471\n",
      "1.61873\n",
      "1.60101\n",
      "1.53297\n",
      "1.62544\n",
      "1.61186\n",
      "1.65158\n",
      "1.57752\n",
      "1.47826\n",
      "1.53166\n",
      "1.56177\n",
      "1.57965\n",
      "1.49241\n",
      "1.45954\n",
      "1.48968\n",
      "1.44551\n",
      "1.50189\n",
      "1.46841\n",
      "1.4856\n",
      "1.43632\n",
      "1.52272\n",
      "1.23556\n",
      "1.32347\n",
      "1.34703\n",
      "1.36236\n",
      "1.41444\n",
      "1.31946\n",
      "1.22409\n",
      "1.32505\n",
      "1.40083\n",
      "1.07419\n",
      "1.32649\n",
      "1.48893\n",
      "1.31036\n",
      "1.3453\n",
      "1.15855\n",
      "1.19727\n",
      "1.10339\n",
      "1.26187\n",
      "1.31377\n",
      "1.2128\n",
      "1.27775\n",
      "1.15201\n",
      "1.23226\n",
      "1.35475\n",
      "1.32413\n",
      "1.18265\n",
      "1.17598\n",
      "1.14323\n",
      "1.18292\n",
      "1.28645\n",
      "1.14369\n",
      "1.19875\n",
      "1.13438\n",
      "1.1632\n",
      "1.12069\n",
      "1.082\n",
      "1.30564\n",
      "1.10301\n",
      "1.07817\n",
      "1.144\n",
      "0.897349\n",
      "1.00785\n",
      "1.16768\n",
      "1.05524\n",
      "1.0918\n",
      "1.09758\n",
      "1.09702\n",
      "1.24504\n",
      "0.906312\n",
      "1.0311\n",
      "1.07718\n",
      "1.02053\n",
      "0.998182\n",
      "1.04402\n",
      "0.993419\n",
      "0.991941\n",
      "1.14191\n",
      "0.938513\n",
      "0.843649\n",
      "1.19348\n",
      "0.985856\n",
      "0.94858\n",
      "0.858477\n",
      "1.0127\n",
      "0.984842\n",
      "0.918935\n",
      "0.946163\n",
      "1.13481\n",
      "1.08925\n",
      "0.976389\n",
      "1.1916\n",
      "0.947718\n",
      "0.954977\n",
      "0.848203\n",
      "0.866538\n",
      "0.915041\n",
      "0.721377\n",
      "0.935212\n",
      "0.952238\n",
      "1.0389\n",
      "0.946196\n",
      "0.903186\n",
      "0.707741\n",
      "0.821301\n",
      "1.0568\n",
      "0.799305\n",
      "0.865328\n",
      "0.983113\n",
      "0.735897\n",
      "1.01693\n",
      "1.08543\n",
      "0.801748\n",
      "0.826947\n",
      "0.969123\n",
      "0.915828\n",
      "1.05462\n",
      "0.758662\n",
      "0.997297\n",
      "0.722598\n",
      "0.885068\n",
      "0.928433\n",
      "1.03598\n",
      "0.934017\n",
      "0.872271\n",
      "0.908755\n",
      "0.91424\n",
      "0.936045\n",
      "0.837639\n",
      "0.811586\n",
      "0.578922\n",
      "0.809793\n",
      "0.753695\n",
      "0.697421\n",
      "0.795558\n",
      "0.893324\n",
      "0.808684\n",
      "0.764252\n",
      "1.00953\n",
      "0.798881\n",
      "0.782488\n",
      "0.759949\n",
      "0.95592\n",
      "1.03017\n",
      "0.807531\n",
      "0.909733\n",
      "0.684146\n",
      "0.916361\n",
      "0.747893\n",
      "0.86189\n",
      "0.828181\n",
      "0.759393\n",
      "1.02621\n",
      "0.815694\n",
      "0.642267\n",
      "0.716773\n",
      "0.69224\n",
      "0.756102\n",
      "0.833949\n",
      "0.843633\n",
      "0.771811\n",
      "0.87318\n",
      "0.605626\n",
      "0.730752\n",
      "0.698512\n",
      "0.719985\n",
      "0.949793\n",
      "0.928047\n",
      "0.747657\n",
      "0.776002\n",
      "0.723697\n",
      "0.68266\n",
      "0.708461\n",
      "0.972712\n",
      "0.676564\n",
      "0.909427\n",
      "0.768945\n",
      "0.742543\n",
      "1.00501\n",
      "0.725344\n",
      "0.776867\n",
      "0.701782\n",
      "0.692508\n",
      "0.582177\n",
      "0.732574\n",
      "0.623855\n",
      "0.632248\n",
      "0.674644\n",
      "1.00786\n",
      "0.613858\n",
      "0.787405\n",
      "0.952397\n",
      "0.908355\n",
      "0.900315\n",
      "1.19233\n",
      "0.908401\n",
      "0.770576\n",
      "0.851552\n",
      "0.81357\n",
      "0.684688\n",
      "0.668773\n",
      "0.838793\n",
      "0.764066\n",
      "0.696781\n",
      "0.876903\n",
      "0.651584\n",
      "0.821337\n",
      "0.649282\n",
      "0.583826\n",
      "0.708209\n",
      "0.745065\n",
      "0.82861\n",
      "0.76642\n",
      "0.74019\n",
      "0.44728\n",
      "0.849347\n",
      "0.509395\n",
      "0.641383\n",
      "0.723957\n",
      "0.673125\n",
      "0.622152\n",
      "0.689486\n",
      "0.697673\n",
      "0.684107\n",
      "0.838666\n",
      "0.634959\n",
      "0.607898\n",
      "0.796629\n",
      "0.656797\n",
      "0.720569\n",
      "0.626869\n",
      "0.582895\n",
      "0.659917\n",
      "0.456742\n",
      "0.657428\n",
      "0.709159\n",
      "0.650052\n",
      "0.510738\n",
      "0.77124\n",
      "0.484945\n",
      "0.595996\n",
      "0.475753\n",
      "0.729227\n",
      "0.601234\n",
      "0.573089\n",
      "0.816667\n",
      "0.78248\n",
      "0.837963\n",
      "0.824702\n",
      "0.723085\n",
      "0.44815\n",
      "0.693184\n",
      "0.768107\n",
      "0.573321\n",
      "0.787947\n",
      "0.620806\n",
      "0.550035\n",
      "0.642688\n",
      "0.619055\n",
      "0.775851\n",
      "0.761612\n",
      "0.6225\n",
      "0.607389\n",
      "0.69895\n",
      "0.642562\n",
      "0.87483\n",
      "0.788182\n",
      "0.713074\n",
      "0.52265\n",
      "0.596083\n",
      "0.794397\n",
      "0.68935\n",
      "0.535344\n",
      "0.464022\n",
      "0.697315\n",
      "0.826615\n",
      "0.601531\n",
      "0.448303\n",
      "0.658204\n",
      "0.733005\n",
      "0.809797\n",
      "0.683656\n",
      "0.657252\n",
      "0.68472\n",
      "0.52917\n",
      "0.630484\n",
      "0.695831\n",
      "0.688659\n",
      "0.549499\n",
      "0.596449\n",
      "0.639446\n",
      "0.62435\n",
      "0.664263\n",
      "0.665238\n",
      "0.649256\n",
      "0.562359\n",
      "0.57426\n",
      "0.624442\n",
      "0.641305\n",
      "0.727254\n",
      "0.451304\n",
      "0.743853\n",
      "0.531522\n",
      "0.589039\n",
      "0.663531\n",
      "0.550459\n",
      "0.726407\n",
      "0.583471\n",
      "0.693402\n",
      "0.612028\n",
      "0.655141\n",
      "0.60924\n",
      "0.749185\n",
      "0.509869\n",
      "0.473704\n",
      "0.531472\n",
      "0.671826\n",
      "0.601628\n",
      "0.510559\n",
      "0.670843\n",
      "0.586889\n",
      "0.621618\n",
      "0.594143\n",
      "0.550513\n",
      "0.634342\n",
      "0.473408\n",
      "0.451968\n",
      "0.689587\n",
      "0.58002\n",
      "0.782663\n",
      "0.674114\n",
      "0.597848\n",
      "0.451898\n",
      "0.494176\n",
      "0.718695\n",
      "0.583049\n",
      "0.575633\n",
      "0.568389\n",
      "0.632065\n",
      "0.612799\n",
      "0.765993\n",
      "0.690413\n",
      "0.489597\n",
      "0.546287\n",
      "0.649486\n",
      "0.66984\n",
      "0.423193\n",
      "0.648758\n",
      "0.55817\n",
      "0.585665\n",
      "0.671733\n",
      "0.551163\n",
      "0.504141\n",
      "0.53775\n",
      "0.49024\n",
      "0.504941\n",
      "0.465423\n",
      "0.60195\n",
      "0.487247\n",
      "0.715711\n",
      "0.679847\n",
      "0.526588\n",
      "0.416732\n",
      "0.572215\n",
      "0.640493\n",
      "0.587848\n",
      "0.454586\n",
      "0.414752\n",
      "0.518249\n",
      "0.851023\n",
      "0.557355\n",
      "0.504927\n",
      "0.602117\n",
      "0.61408\n",
      "0.563978\n",
      "0.421933\n",
      "0.574665\n",
      "0.550218\n",
      "0.554087\n",
      "0.44713\n",
      "0.55467\n",
      "0.576511\n",
      "0.663037\n",
      "0.393225\n",
      "0.483232\n",
      "0.540468\n",
      "0.505608\n",
      "0.455559\n",
      "0.480454\n",
      "0.617795\n",
      "0.369705\n",
      "0.383882\n",
      "0.561379\n",
      "0.589131\n",
      "0.547487\n",
      "0.729648\n",
      "0.605611\n",
      "0.433539\n",
      "0.564687\n",
      "0.516751\n",
      "0.718431\n",
      "0.697167\n",
      "0.478151\n",
      "0.595606\n",
      "0.424095\n",
      "0.407537\n",
      "0.472994\n",
      "0.601236\n",
      "0.444645\n",
      "0.572085\n",
      "0.509785\n",
      "0.51213\n",
      "0.395855\n",
      "0.461834\n",
      "0.593017\n",
      "0.661182\n",
      "0.613859\n",
      "0.499292\n",
      "0.56624\n",
      "0.768073\n",
      "0.551197\n",
      "0.61957\n",
      "0.364629\n",
      "0.561103\n",
      "0.481077\n",
      "0.668305\n",
      "0.519042\n",
      "0.482059\n",
      "0.748523\n",
      "0.399439\n",
      "0.36037\n",
      "0.475104\n",
      "0.511428\n",
      "0.356353\n",
      "0.484735\n",
      "0.451975\n",
      "0.570746\n",
      "0.403728\n",
      "0.398022\n",
      "0.45052\n",
      "0.509726\n",
      "0.636853\n",
      "0.491076\n",
      "0.417586\n",
      "0.576643\n",
      "0.48016\n",
      "0.435375\n",
      "0.616749\n",
      "0.430548\n",
      "0.673517\n",
      "0.339963\n",
      "0.513791\n",
      "0.349959\n",
      "0.481903\n",
      "0.623818\n",
      "0.671748\n",
      "0.308358\n",
      "0.496782\n",
      "0.442244\n",
      "0.553039\n",
      "0.486175\n",
      "0.401004\n",
      "0.592089\n",
      "0.589386\n",
      "0.542889\n",
      "0.576491\n",
      "0.560515\n",
      "0.49975\n",
      "0.501789\n",
      "0.432975\n",
      "0.335494\n",
      "0.4449\n",
      "0.32426\n",
      "0.410159\n",
      "0.480075\n",
      "0.509909\n",
      "0.58799\n",
      "0.538367\n",
      "0.362695\n",
      "0.427606\n",
      "0.649603\n",
      "0.532322\n",
      "0.357651\n",
      "0.483773\n",
      "0.562905\n",
      "0.672118\n",
      "0.568714\n",
      "0.533992\n",
      "0.552002\n",
      "0.536307\n",
      "0.877456\n",
      "0.563748\n",
      "0.492647\n",
      "0.511166\n",
      "0.461748\n",
      "0.557191\n",
      "0.433246\n",
      "0.435599\n",
      "0.393292\n",
      "0.323992\n",
      "0.415907\n",
      "0.511509\n",
      "0.533059\n",
      "0.661892\n",
      "0.65126\n",
      "0.505061\n",
      "0.685402\n",
      "0.558323\n",
      "0.577151\n",
      "0.443986\n",
      "0.47586\n",
      "0.39956\n",
      "0.59622\n",
      "0.424887\n",
      "0.493811\n",
      "0.389376\n",
      "0.483036\n",
      "0.618644\n",
      "0.500129\n",
      "0.563331\n",
      "0.423376\n",
      "0.491433\n",
      "0.38339\n",
      "0.506412\n",
      "0.436101\n",
      "0.452916\n",
      "0.282043\n",
      "0.409799\n",
      "0.516632\n",
      "0.502759\n",
      "0.423067\n",
      "0.476792\n",
      "0.531936\n",
      "0.437869\n",
      "0.54966\n",
      "0.626159\n",
      "0.467608\n",
      "0.438318\n",
      "0.543355\n",
      "0.475052\n",
      "0.469205\n",
      "0.316339\n",
      "0.422518\n",
      "0.418113\n",
      "0.345442\n",
      "0.534424\n",
      "0.432229\n",
      "0.440433\n",
      "0.377136\n",
      "0.394644\n",
      "0.501643\n",
      "0.443933\n",
      "0.391123\n",
      "0.413373\n",
      "0.386562\n",
      "0.555253\n",
      "0.463795\n",
      "0.383136\n",
      "0.349244\n",
      "0.382257\n",
      "0.415425\n",
      "0.377131\n",
      "0.529752\n",
      "0.41898\n",
      "0.389829\n",
      "0.254964\n",
      "0.444587\n",
      "0.324923\n",
      "0.429714\n",
      "0.478198\n",
      "0.427553\n",
      "0.525038\n",
      "0.475768\n",
      "0.379074\n",
      "0.328981\n",
      "0.32656\n",
      "0.37651\n",
      "0.3776\n",
      "0.416395\n",
      "0.621997\n",
      "0.394137\n",
      "0.413914\n",
      "0.520848\n",
      "0.449012\n",
      "0.401603\n",
      "0.551206\n",
      "0.590734\n",
      "0.437293\n",
      "0.455091\n",
      "0.236274\n",
      "0.438713\n",
      "0.500398\n",
      "0.491245\n",
      "0.436689\n",
      "0.325989\n",
      "0.545957\n",
      "0.36487\n",
      "0.492105\n",
      "0.420685\n",
      "0.44929\n",
      "0.303157\n",
      "0.359787\n",
      "0.396097\n",
      "0.353628\n",
      "0.404195\n",
      "0.539646\n",
      "0.280403\n",
      "0.446659\n",
      "0.371103\n",
      "0.515781\n",
      "0.404792\n",
      "0.535862\n",
      "0.402579\n",
      "0.528622\n",
      "0.353511\n",
      "0.435316\n",
      "0.248843\n",
      "0.498421\n",
      "0.435321\n",
      "0.430193\n",
      "0.270579\n",
      "0.304991\n",
      "0.414262\n",
      "0.310782\n",
      "0.341427\n",
      "0.359885\n",
      "0.664485\n",
      "0.36444\n",
      "0.405257\n",
      "0.467987\n",
      "0.419575\n",
      "0.434584\n",
      "0.381905\n",
      "0.234986\n",
      "0.560526\n",
      "0.405916\n",
      "0.416232\n",
      "0.335739\n",
      "0.450818\n",
      "0.518231\n",
      "0.518557\n",
      "0.364283\n",
      "0.365222\n",
      "0.425122\n",
      "0.247671\n",
      "0.489081\n",
      "0.375744\n",
      "0.303016\n",
      "0.34421\n",
      "0.459817\n",
      "0.278723\n",
      "0.33822\n",
      "0.383771\n",
      "0.541536\n",
      "0.443471\n",
      "0.295285\n",
      "0.382259\n",
      "0.365036\n",
      "0.36601\n",
      "0.358158\n",
      "0.447269\n",
      "0.39683\n",
      "0.361089\n",
      "0.426979\n",
      "0.352496\n",
      "0.345585\n",
      "0.392808\n",
      "0.306178\n",
      "0.381159\n",
      "0.5857\n",
      "0.293266\n",
      "0.350222\n",
      "0.412191\n",
      "0.495568\n",
      "0.296479\n",
      "0.359292\n",
      "0.283023\n",
      "0.326193\n",
      "0.336556\n",
      "0.411593\n",
      "0.316926\n",
      "0.41557\n",
      "0.433795\n",
      "0.384338\n",
      "0.334502\n",
      "0.480639\n",
      "0.401217\n",
      "0.46844\n",
      "0.299977\n",
      "0.321064\n",
      "0.263504\n",
      "0.334745\n",
      "0.300662\n",
      "0.425773\n",
      "0.303157\n",
      "0.308394\n",
      "0.426527\n",
      "0.430456\n",
      "0.429518\n",
      "0.515622\n",
      "0.43425\n",
      "0.344905\n",
      "0.265905\n",
      "0.427954\n",
      "0.424547\n",
      "0.444979\n",
      "0.426321\n",
      "0.272935\n",
      "0.464236\n",
      "0.300585\n",
      "0.299697\n",
      "0.520523\n",
      "0.371868\n",
      "0.35153\n",
      "0.418476\n",
      "0.294967\n",
      "0.398044\n",
      "0.345847\n",
      "0.286493\n",
      "0.43239\n",
      "0.240685\n",
      "0.292606\n",
      "0.324817\n",
      "0.433981\n",
      "0.435866\n",
      "0.358464\n",
      "0.402911\n",
      "0.371117\n",
      "0.360458\n",
      "0.54052\n",
      "0.290971\n",
      "0.307812\n",
      "0.273703\n",
      "0.368529\n",
      "0.480561\n",
      "0.358213\n",
      "0.288326\n",
      "0.420374\n",
      "0.393946\n",
      "0.323786\n",
      "0.552094\n",
      "0.653181\n",
      "0.373211\n",
      "0.457564\n",
      "0.564154\n",
      "0.274393\n",
      "0.477986\n",
      "0.542741\n",
      "0.537237\n",
      "0.439823\n",
      "0.453497\n",
      "0.404022\n",
      "0.420002\n",
      "0.228011\n",
      "0.219218\n",
      "0.599674\n",
      "0.337908\n",
      "0.35103\n",
      "0.459171\n",
      "0.439109\n",
      "0.388898\n",
      "0.496614\n",
      "0.406952\n",
      "0.398548\n",
      "0.404672\n",
      "0.30327\n",
      "0.262845\n",
      "0.398892\n",
      "0.33583\n",
      "0.193549\n",
      "0.286461\n",
      "0.368484\n",
      "0.349312\n",
      "0.443491\n",
      "0.257715\n",
      "0.31549\n",
      "0.379673\n",
      "0.285666\n",
      "0.476847\n",
      "0.232185\n",
      "0.365866\n",
      "0.4853\n",
      "0.268576\n",
      "0.518686\n",
      "0.334358\n",
      "0.184685\n",
      "0.250609\n",
      "0.422054\n",
      "0.294828\n",
      "0.407006\n",
      "0.454415\n",
      "0.244132\n",
      "0.25742\n",
      "0.23578\n",
      "0.430141\n",
      "0.368614\n",
      "0.320977\n",
      "0.35451\n",
      "0.354594\n",
      "0.34496\n",
      "0.36348\n",
      "0.373389\n",
      "0.445636\n",
      "0.382453\n",
      "0.265975\n",
      "0.30132\n",
      "0.341265\n",
      "0.286376\n",
      "0.535555\n",
      "0.363006\n",
      "0.425839\n",
      "0.193301\n",
      "0.372277\n",
      "0.305243\n",
      "0.362495\n",
      "0.280267\n",
      "0.315522\n",
      "0.356172\n",
      "0.285221\n",
      "0.308835\n",
      "0.631833\n",
      "0.25254\n",
      "0.490624\n",
      "0.296874\n",
      "0.537958\n",
      "0.403603\n",
      "0.29919\n",
      "0.366391\n",
      "0.392199\n",
      "0.421267\n",
      "0.401292\n",
      "0.315543\n",
      "0.439859\n",
      "0.233443\n",
      "0.335699\n",
      "0.336853\n",
      "0.314625\n",
      "0.2706\n",
      "0.225001\n",
      "0.247647\n",
      "0.383796\n",
      "0.307366\n",
      "0.434447\n",
      "0.391386\n",
      "0.448424\n",
      "0.300555\n",
      "0.261861\n",
      "0.375892\n",
      "0.314915\n",
      "0.369681\n",
      "0.300077\n",
      "0.35577\n",
      "0.312631\n",
      "0.253004\n",
      "0.314907\n",
      "0.30719\n",
      "0.295605\n",
      "0.275043\n",
      "0.39036\n",
      "0.484896\n",
      "0.165855\n",
      "0.336504\n",
      "0.355497\n",
      "0.266257\n",
      "0.166788\n",
      "0.274246\n",
      "0.470995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.226539\n",
      "0.34423\n",
      "0.345852\n",
      "0.413636\n",
      "0.38211\n",
      "0.432408\n",
      "0.399583\n",
      "0.419063\n",
      "0.333042\n",
      "0.450993\n",
      "0.413004\n",
      "0.233742\n",
      "0.380966\n",
      "0.589748\n",
      "0.425509\n",
      "0.524244\n",
      "0.36052\n",
      "0.591642\n",
      "0.520113\n",
      "0.422214\n",
      "0.26661\n",
      "0.308396\n",
      "0.58618\n",
      "0.332824\n",
      "0.352794\n",
      "0.32443\n",
      "0.494096\n",
      "0.351189\n",
      "0.268869\n",
      "0.412584\n",
      "0.374212\n",
      "0.361223\n",
      "0.384332\n",
      "0.283075\n",
      "0.407787\n",
      "0.358074\n",
      "0.572793\n",
      "0.369052\n",
      "0.449417\n",
      "0.318758\n",
      "0.216924\n",
      "0.514087\n",
      "0.328652\n",
      "0.330582\n",
      "0.37431\n",
      "0.324288\n",
      "0.200793\n",
      "0.334558\n",
      "0.220123\n",
      "0.223206\n",
      "0.298245\n",
      "0.414975\n",
      "0.460582\n",
      "0.358872\n",
      "0.406928\n",
      "0.362229\n",
      "0.233422\n",
      "0.316652\n",
      "0.302709\n",
      "0.223948\n",
      "0.334522\n",
      "0.28847\n",
      "0.337002\n",
      "0.263398\n",
      "0.331727\n",
      "0.267855\n",
      "0.342862\n",
      "0.3315\n",
      "0.21702\n",
      "0.258405\n",
      "0.312387\n",
      "0.354433\n",
      "0.4372\n",
      "0.208263\n",
      "0.384881\n",
      "0.32877\n",
      "0.292959\n",
      "0.337818\n",
      "0.287039\n",
      "0.359938\n",
      "0.275441\n",
      "0.396324\n",
      "0.280146\n",
      "0.161788\n",
      "0.229932\n",
      "0.28466\n",
      "0.211247\n",
      "0.216193\n",
      "0.25016\n",
      "0.30305\n",
      "0.298103\n",
      "0.280066\n",
      "0.32919\n",
      "0.387405\n",
      "0.314427\n",
      "0.213783\n",
      "0.164205\n",
      "0.400664\n",
      "0.329625\n",
      "0.321399\n",
      "0.183785\n",
      "0.220574\n",
      "0.213564\n",
      "0.229147\n",
      "0.403537\n",
      "0.219135\n",
      "0.278229\n",
      "0.273353\n",
      "0.458195\n",
      "0.325476\n",
      "0.367576\n",
      "0.415032\n",
      "0.354505\n",
      "0.329219\n",
      "0.24962\n",
      "0.367374\n",
      "0.221594\n",
      "0.255508\n",
      "0.259308\n",
      "0.266961\n",
      "0.307089\n",
      "0.363332\n",
      "0.452486\n",
      "0.371144\n",
      "0.292616\n",
      "0.318346\n",
      "0.266437\n",
      "0.18664\n",
      "0.372436\n",
      "0.290731\n",
      "0.215043\n",
      "0.312633\n",
      "0.180887\n",
      "0.273354\n",
      "0.198265\n",
      "0.218488\n",
      "0.305642\n",
      "0.398562\n",
      "0.310499\n",
      "0.211511\n",
      "0.283539\n",
      "0.187778\n",
      "0.234437\n",
      "0.534198\n",
      "0.361707\n",
      "0.417697\n",
      "0.278442\n",
      "0.336545\n",
      "0.413528\n",
      "0.353871\n",
      "0.262185\n",
      "0.146679\n",
      "0.21753\n",
      "0.469262\n",
      "0.304002\n",
      "0.311748\n",
      "0.268115\n",
      "0.337631\n",
      "0.251805\n",
      "0.40398\n",
      "0.357523\n",
      "0.412957\n",
      "0.272792\n",
      "0.410141\n",
      "0.291988\n",
      "0.199664\n",
      "0.456512\n",
      "0.223025\n",
      "0.321622\n",
      "0.215314\n",
      "0.188697\n",
      "0.379623\n",
      "0.243063\n",
      "0.384838\n",
      "0.467204\n",
      "0.25647\n",
      "0.412196\n",
      "0.410805\n",
      "0.505779\n",
      "0.157254\n",
      "0.30787\n",
      "0.357441\n",
      "0.196236\n",
      "0.331749\n",
      "0.614259\n",
      "0.386389\n",
      "0.348755\n",
      "0.237191\n",
      "0.366876\n",
      "0.730476\n",
      "0.222186\n",
      "0.300921\n",
      "0.620831\n",
      "0.605489\n",
      "0.382394\n",
      "0.376006\n",
      "0.384254\n",
      "0.228337\n",
      "0.295398\n",
      "0.590534\n",
      "0.287424\n",
      "0.352045\n",
      "0.231979\n",
      "0.333655\n",
      "0.242127\n",
      "0.376135\n",
      "0.275106\n",
      "0.36535\n",
      "0.322199\n",
      "0.445986\n",
      "0.429868\n",
      "0.240746\n",
      "0.148524\n",
      "0.329467\n",
      "0.50855\n",
      "0.207193\n",
      "0.165784\n",
      "0.194542\n",
      "0.280965\n",
      "0.352925\n",
      "0.20543\n",
      "0.404111\n",
      "0.266458\n",
      "0.37216\n",
      "0.398934\n",
      "0.315497\n",
      "0.299377\n",
      "0.303594\n",
      "0.323015\n",
      "0.21451\n",
      "0.389555\n",
      "0.436092\n",
      "0.107581\n",
      "0.303541\n",
      "0.238574\n",
      "0.542423\n",
      "0.224893\n",
      "0.264675\n",
      "0.309894\n",
      "0.513944\n",
      "0.340626\n",
      "0.154219\n",
      "0.23021\n",
      "0.271805\n",
      "0.405778\n",
      "0.363125\n",
      "0.321241\n",
      "0.419311\n",
      "0.248623\n",
      "0.375661\n",
      "0.24101\n",
      "0.242419\n",
      "0.164645\n",
      "0.408729\n",
      "0.241389\n",
      "0.241684\n",
      "0.586977\n",
      "0.351931\n",
      "0.312438\n",
      "0.461028\n",
      "0.327049\n",
      "0.246464\n",
      "0.250271\n",
      "0.190903\n",
      "0.241468\n",
      "0.178341\n",
      "0.23101\n",
      "0.256858\n",
      "0.288175\n",
      "0.255343\n",
      "0.331494\n",
      "0.207163\n",
      "0.305651\n",
      "0.227732\n",
      "0.286058\n",
      "0.271204\n",
      "0.349971\n",
      "0.1228\n",
      "0.213275\n",
      "0.342909\n",
      "0.240099\n",
      "0.282886\n",
      "0.357962\n",
      "0.268493\n",
      "0.162154\n",
      "0.115617\n",
      "0.218507\n",
      "0.341512\n",
      "0.304957\n",
      "0.136239\n",
      "0.269556\n",
      "0.428928\n",
      "0.271772\n",
      "0.231929\n",
      "0.244376\n",
      "0.266013\n",
      "0.283724\n",
      "0.253991\n",
      "0.243123\n",
      "0.236529\n",
      "0.208238\n",
      "0.377343\n",
      "0.415521\n",
      "0.313336\n",
      "0.177956\n",
      "0.227001\n",
      "0.28609\n",
      "0.306796\n",
      "0.368754\n",
      "0.197102\n",
      "0.183886\n",
      "0.251035\n",
      "0.267752\n",
      "0.255964\n",
      "0.325858\n",
      "0.272736\n",
      "0.315848\n",
      "0.189164\n",
      "0.264027\n",
      "0.197021\n",
      "0.279704\n",
      "0.31808\n",
      "0.314999\n",
      "0.290691\n",
      "0.196666\n",
      "0.288175\n",
      "0.427943\n",
      "0.406437\n",
      "0.383713\n",
      "0.251733\n",
      "0.30188\n",
      "0.47165\n",
      "0.29589\n",
      "0.225425\n",
      "0.244351\n",
      "0.253649\n",
      "0.184325\n",
      "0.224204\n",
      "0.23234\n",
      "0.236778\n",
      "0.456593\n",
      "0.211143\n",
      "0.202597\n",
      "0.367107\n",
      "0.232318\n",
      "0.265511\n",
      "0.167387\n",
      "0.351372\n",
      "0.445415\n",
      "0.308199\n",
      "0.583315\n",
      "0.374644\n",
      "0.275405\n",
      "0.219129\n",
      "0.317572\n",
      "0.340045\n",
      "0.287068\n",
      "0.212434\n",
      "0.232648\n",
      "0.149318\n",
      "0.224219\n",
      "0.412181\n",
      "0.273855\n",
      "0.276504\n",
      "0.230291\n",
      "0.40102\n",
      "0.244051\n",
      "0.299292\n",
      "0.265922\n",
      "0.270811\n",
      "0.13893\n",
      "0.167255\n",
      "0.223889\n",
      "0.284286\n",
      "0.39646\n",
      "0.183833\n",
      "0.408857\n",
      "0.15908\n",
      "0.190704\n",
      "0.200612\n",
      "0.37415\n",
      "0.201035\n",
      "0.282654\n",
      "0.237398\n",
      "0.220988\n",
      "0.669476\n",
      "0.19932\n",
      "0.17921\n",
      "0.154331\n",
      "0.30204\n",
      "0.261389\n",
      "0.270367\n",
      "0.18024\n",
      "0.306429\n",
      "0.189649\n",
      "0.211349\n",
      "0.332451\n",
      "0.179053\n",
      "0.206922\n",
      "0.340835\n",
      "0.415631\n",
      "0.273655\n",
      "0.078953\n",
      "0.210752\n",
      "0.333864\n",
      "0.234597\n",
      "0.346507\n",
      "0.219793\n",
      "0.168383\n",
      "0.337508\n",
      "0.2068\n",
      "0.191546\n",
      "0.320002\n",
      "0.227577\n",
      "0.418703\n",
      "0.193412\n",
      "0.158842\n",
      "0.362684\n",
      "0.186413\n",
      "0.319566\n",
      "0.244081\n",
      "0.0990401\n",
      "0.315245\n",
      "0.312077\n",
      "0.218239\n",
      "0.366366\n",
      "0.16536\n",
      "0.217236\n",
      "0.272866\n",
      "0.188591\n",
      "0.185326\n",
      "0.284867\n",
      "0.250189\n",
      "0.225124\n",
      "0.192075\n",
      "0.136538\n",
      "0.184103\n",
      "0.212589\n",
      "0.178207\n",
      "0.23685\n",
      "0.214793\n",
      "0.252617\n",
      "0.357339\n",
      "0.239331\n",
      "0.16791\n",
      "0.239406\n",
      "0.20005\n",
      "0.271482\n",
      "0.347412\n",
      "0.310127\n",
      "0.385334\n",
      "0.165954\n",
      "0.229728\n",
      "0.215152\n",
      "0.131669\n",
      "0.216975\n",
      "0.285626\n",
      "0.189374\n",
      "0.172857\n",
      "0.210701\n",
      "0.172887\n",
      "0.229954\n",
      "0.429947\n",
      "0.212805\n",
      "0.264661\n",
      "0.19539\n",
      "0.371714\n",
      "0.19706\n",
      "0.244505\n",
      "0.24125\n",
      "0.313488\n",
      "0.327199\n",
      "0.305357\n",
      "0.218725\n",
      "0.229683\n",
      "0.173877\n",
      "0.206239\n",
      "0.251164\n",
      "0.152569\n",
      "0.292811\n",
      "0.179621\n",
      "0.215821\n",
      "0.207835\n",
      "0.243907\n",
      "0.258692\n",
      "0.240892\n",
      "0.0842947\n",
      "0.32842\n",
      "0.180131\n",
      "0.205203\n",
      "0.298396\n",
      "0.346654\n",
      "0.188347\n",
      "0.131888\n",
      "0.226688\n",
      "0.372998\n",
      "0.236501\n",
      "0.20179\n",
      "0.160735\n",
      "0.152269\n",
      "0.276687\n",
      "0.340129\n",
      "0.235744\n",
      "0.170914\n",
      "0.334997\n",
      "0.228667\n",
      "0.247405\n",
      "0.248552\n",
      "0.358028\n",
      "0.160049\n",
      "0.121645\n",
      "0.146593\n",
      "0.295758\n",
      "0.16124\n",
      "0.182768\n",
      "0.180292\n",
      "0.341909\n",
      "0.185987\n",
      "0.185095\n",
      "0.263893\n",
      "0.252436\n",
      "0.181436\n",
      "0.317927\n",
      "0.235277\n",
      "0.201777\n",
      "0.0778368\n",
      "0.122747\n",
      "0.294324\n",
      "0.166488\n",
      "0.254041\n",
      "0.228059\n",
      "0.134806\n",
      "0.204022\n",
      "0.272352\n",
      "0.204182\n",
      "0.331131\n",
      "0.180214\n",
      "0.167221\n",
      "0.174359\n",
      "0.258421\n",
      "0.201461\n",
      "0.123241\n",
      "0.196626\n",
      "0.177335\n",
      "0.352505\n",
      "0.142341\n",
      "0.14899\n",
      "0.359898\n",
      "0.16566\n",
      "0.245849\n",
      "0.178454\n",
      "0.126063\n",
      "0.166296\n",
      "0.161834\n",
      "0.188511\n",
      "0.107875\n",
      "0.114901\n",
      "0.267624\n",
      "0.150615\n",
      "0.209459\n",
      "0.203315\n",
      "0.208183\n",
      "0.393865\n",
      "0.234967\n",
      "0.174862\n",
      "0.194352\n",
      "0.212434\n",
      "0.122\n",
      "0.20125\n",
      "0.346219\n",
      "0.152977\n",
      "0.225555\n",
      "0.210108\n",
      "0.152326\n",
      "0.197032\n",
      "0.200497\n",
      "0.309495\n",
      "0.133779\n",
      "0.257488\n",
      "0.15582\n",
      "0.214601\n",
      "0.120358\n",
      "0.197855\n",
      "0.0973798\n",
      "0.210749\n",
      "0.200463\n",
      "0.232703\n",
      "0.199808\n",
      "0.0877656\n",
      "0.107175\n",
      "0.175046\n",
      "0.2205\n",
      "0.191068\n",
      "0.150377\n",
      "0.192608\n",
      "0.217226\n",
      "0.163641\n",
      "0.346418\n",
      "0.156708\n",
      "0.114248\n",
      "0.357978\n",
      "0.16371\n",
      "0.169558\n",
      "0.1579\n",
      "0.107174\n",
      "0.209159\n",
      "0.232947\n",
      "0.138253\n",
      "0.109142\n",
      "0.204302\n",
      "0.181573\n",
      "0.127099\n",
      "0.168312\n",
      "0.288201\n",
      "0.245559\n",
      "0.219403\n",
      "0.148454\n",
      "0.13992\n",
      "0.23775\n",
      "0.230006\n",
      "0.219136\n",
      "0.20716\n",
      "0.304989\n",
      "0.155548\n",
      "0.191557\n",
      "0.119043\n",
      "0.155914\n",
      "0.193143\n",
      "0.209334\n",
      "0.116529\n",
      "0.160063\n",
      "0.0771188\n",
      "0.127973\n",
      "0.274042\n",
      "0.159889\n",
      "0.207889\n",
      "0.264576\n",
      "0.191631\n",
      "0.208071\n",
      "0.233982\n",
      "0.130124\n",
      "0.216674\n",
      "0.289095\n",
      "0.124398\n",
      "0.299144\n",
      "0.14008\n",
      "0.127595\n",
      "0.259377\n",
      "0.147495\n",
      "0.161228\n",
      "0.157801\n",
      "0.159179\n",
      "0.122671\n",
      "0.149691\n",
      "0.172038\n",
      "0.126608\n",
      "0.172282\n",
      "0.173224\n",
      "0.165079\n",
      "0.133442\n",
      "0.119044\n",
      "0.154079\n",
      "0.118947\n",
      "0.113028\n",
      "0.109516\n",
      "0.118683\n",
      "0.183019\n",
      "0.14303\n",
      "0.195475\n",
      "0.118636\n",
      "0.129647\n",
      "0.235215\n",
      "0.234453\n",
      "0.246436\n",
      "0.0997462\n",
      "0.10857\n",
      "0.26302\n",
      "0.140523\n",
      "0.139303\n",
      "0.141091\n",
      "0.178324\n",
      "0.174951\n",
      "0.163183\n",
      "0.231211\n",
      "0.132772\n",
      "0.231355\n",
      "0.150509\n",
      "0.160546\n",
      "0.2009\n",
      "0.212876\n",
      "0.0783439\n",
      "0.21626\n",
      "0.237277\n",
      "0.10038\n",
      "0.164324\n",
      "0.229962\n",
      "0.198945\n",
      "0.156037\n",
      "0.232531\n",
      "0.252324\n",
      "0.228098\n",
      "0.186105\n",
      "0.147855\n",
      "0.273971\n",
      "0.174046\n",
      "0.29789\n",
      "0.283029\n",
      "0.130369\n",
      "0.228658\n",
      "0.275799\n",
      "0.150019\n",
      "0.185463\n",
      "0.143635\n",
      "0.227955\n",
      "0.304106\n",
      "0.153847\n",
      "0.21647\n",
      "0.18016\n",
      "0.164056\n",
      "0.118127\n",
      "0.251924\n",
      "0.171879\n",
      "0.216244\n",
      "0.253551\n",
      "0.145796\n",
      "0.182463\n",
      "0.412961\n",
      "0.106277\n",
      "0.170407\n",
      "0.16004\n",
      "0.191646\n",
      "0.224724\n",
      "0.206592\n",
      "0.163846\n",
      "0.120223\n",
      "0.142997\n",
      "0.162269\n",
      "0.217087\n",
      "0.23059\n",
      "0.208169\n",
      "0.0784383\n",
      "0.134305\n",
      "0.13631\n",
      "0.160503\n",
      "0.213254\n",
      "0.126239\n",
      "0.13104\n",
      "0.165381\n",
      "0.127023\n",
      "0.135169\n",
      "0.274174\n",
      "0.0927929\n",
      "0.18546\n",
      "0.140717\n",
      "0.0853898\n",
      "0.14466\n",
      "0.13906\n",
      "0.245309\n",
      "0.0941145\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGJFJREFUeJzt3XuUXWWd5vHvQwAVuUNEIJEgHUejrUuMgEvtpgUdQBTb\nSze0F9Q1k8aWGW+zGARHne52loqOyoAi3oAlineNTBSBae+iBAQUEAm0yCVIRBQQFYO/+ePs0kN5\nquok9VadquT7WWuv2pd37/17c+A8Z1/OPqkqJEmari1GXYAkadNgoEiSmjBQJElNGCiSpCYMFElS\nEwaKJKkJA0UaJ8mXkhw96jqk+cZA0ZyR5CdJDh51HVV1aFWdOeo6AJJ8Ncl/moX9PCDJh5PcmeTW\nJK+dov0/JLkhya+TfD7JzsNuK8mzkvwwyd1Jvp1k2Uz1S7PLQNFmJcmWo65hzFyqBXgzsBTYC/gb\n4LgkhwxqmOTRwPuBFwO7AfcA7x1mW0mWAmcDxwA7Al8EVs6xfwttJANF80KSw5NcluSX3afax/Yt\nOz7JdUnuSnJVkr/tW/bSJN9K8q4ktwNv7uZ9M8k7ktyR5N+THNq3zh+PCoZou3eSr3f7viDJqUk+\nOkEfDkxyU5L/nuRW4CNJdkpybpJ13fbPTbKoa/8W4KnAKd2n+VO6+Y9Mcn6SXyS5JsnfNfgnPhr4\nl6q6o6quBk4HXjpB2xcCX6yqr1fV3cD/AJ6bZLshtvUfgW9W1Teraj3wNmBP4K8b9EEjZqBozkvy\neODDwD8Cu9D7dLwyyQO6JtfRe+PdAfifwEeT7N63if2B6+l9mn5L37xrgF2BtwMfSpIJSpis7ceA\n73V1vZnep/bJPBTYmd6n9xX0/h/8SDf9MOA3wCkAVXUi8A3g2KratqqOTfJg4Pxuvw8BjgTeO9Fp\noyTv7UJ40HBF12YnYHfg8r5VLwcePUEfHt3ftqquA34HPGIjtpVueMwEyzWPGCiaD1YA76+q71bV\nfd31jd8BBwBU1aeq6paq+kNVfQK4Ftivb/1bqur/VNX6qvpNN++GqvpAVd0HnEnvTXC3CfY/sG2S\nhwFPBN5YVfdW1TeBlVP05Q/Am6rqd1X1m6q6vao+U1X3VNVd9AJvsk/rhwM/qaqPdP35PvAZ4AWD\nGlfVP1XVjhMMY0d523Z/f9W36p3Adgy27bi2/e2n2tYFwF93R2tbAycAWwPbTNJnzRMGiuaDvYDX\n9X+6BhYDewAkeUnf6bBf0vu0u2vf+jcO2OatYyNVdU83uu2AdpO13QP4Rd+8ifbVb11V/XZsIsk2\nSd7fXeC+E/g6sGOSBROsvxew/7h/ixfSO/LZWHd3f7fvm7cDcNck7bcfN2+s/aTbqqof0Tsldgqw\nlt7rdBVw00bWrjnEQNF8cCPwlnGfrrepqo8n2Qv4AHAssEtV7Qj8kN5plDEz9UjttcDOSfo/XS+e\nYp3xtbwO+A/A/lW1PfBX3fxM0P5G4Gvj/i22rapXDNpZktO66y+DhisBquqOri+P61v1ccCVE/Th\nyv62Sfahd5Tx42G2VVWfrqrHVNUuwJuAJcDFE+xL84iBorlmqyQP7Bu2pBcYxyTZPz0PTvLM7iLw\ng+m96a4DSPIyZul8fFXdAKymd6F/6yRPAp61gZvZjt51k1+md+vtm8Yt/xnw8L7pc+ldq3hxkq26\n4YlJHjVBjcd0gTNo6L+ucRbwhu4mgUcB/xk4Y4KazwaeleSp3TWdfwE+252ym3JbSZ6QZEGShfQu\n2K/sjlw0zxkommtW0XuDHRveXFWr6b0pnQLcAayhu2uoqq4C3gl8h96b718C35rFel8IPAm4HfhX\n4BP0ru8M693Ag4CfAxcBXx63/D3A87s7wE7u3rSfQe9i/C30Tse9DXgA0/Mmejc33AB8FXh7Vf2x\nlu6I5qkAVXUlvdt+zwZuoxfq/zTstro+/ZLejQ530HtttQmIP7AltZPkE8CPqmr8kYa0yfMIRZqG\n7nTTPkm2SO/Le0cAnx91XdIo+O1UaXoeCnyW3vdQbgJe0d3KK212POUlSWrCU16SpCY2q1Neu+66\nay1ZsmTUZUjSvHLJJZf8vKoWTtVuswqUJUuWsHr16lGXIUnzSpIbhmnnKS9JUhMGiiSpCQNFktSE\ngSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJ\nTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMjDZQkhyS5\nJsmaJMcPWJ4kJ3fLr0iy77jlC5J8P8m5s1e1JGmQkQVKkgXAqcChwDLgqCTLxjU7FFjaDSuA941b\n/irg6hkuVZI0hFEeoewHrKmq66vqXuAc4IhxbY4Azqqei4Adk+wOkGQR8Ezgg7NZtCRpsFEGyp7A\njX3TN3Xzhm3zbuA44A+T7STJiiSrk6xet27d9CqWJE1oXl6UT3I4cFtVXTJV26o6vaqWV9XyhQsX\nzkJ1krR5GmWg3Aws7pte1M0bps2TgWcn+Qm9U2VPS/LRmStVkjSVUQbKxcDSJHsn2Ro4Elg5rs1K\n4CXd3V4HAL+qqrVV9fqqWlRVS7r1/l9VvWhWq5ck3c+Wo9pxVa1PcixwHrAA+HBVXZnkmG75acAq\n4DBgDXAP8LJR1StJmlyqatQ1zJrly5fX6tWrR12GJM0rSS6pquVTtZuXF+UlSXOPgSJJasJAkSQ1\nYaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJ\nUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJA\nkSQ1YaBIkpowUCRJTRgokqQmRhooSQ5Jck2SNUmOH7A8SU7ull+RZN9u/uIk/5bkqiRXJnnV7Fcv\nSeo3skBJsgA4FTgUWAYclWTZuGaHAku7YQXwvm7+euB1VbUMOAB45YB1JUmzaJRHKPsBa6rq+qq6\nFzgHOGJcmyOAs6rnImDHJLtX1dqquhSgqu4Crgb2nM3iJUn3N8pA2RO4sW/6Jv48FKZsk2QJ8Hjg\nu80rlCQNbV5flE+yLfAZ4NVVdecEbVYkWZ1k9bp162a3QEnajIwyUG4GFvdNL+rmDdUmyVb0wuTs\nqvrsRDupqtOranlVLV+4cGGTwiVJf26UgXIxsDTJ3km2Bo4EVo5rsxJ4SXe31wHAr6pqbZIAHwKu\nrqr/PbtlS5IG2XJUO66q9UmOBc4DFgAfrqorkxzTLT8NWAUcBqwB7gFe1q3+ZODFwA+SXNbNO6Gq\nVs1mHyRJf5KqGnUNs2b58uW1evXqUZchSfNKkkuqavlU7eb1RXlJ0txhoEiSmjBQJElNGCiSpCYM\nFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklq\nwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmhgqUJK8YJh5\nkqTN17BHKK8fcp4kaTO15WQLkxwKHAbsmeTkvkXbA+tnsjBJ0vwyaaAAtwCrgWcDl/TNvwt4zUwV\nJUmafyYNlKq6HLg8yceq6vcASXYCFlfVHbNRoCRpfhj2Gsr5SbZPsjNwKfCBJO+a7s6THJLkmiRr\nkhw/YHmSnNwtvyLJvsOuK0maXcMGyg5VdSfwXOCsqtofOGg6O06yADgVOBRYBhyVZNm4ZocCS7th\nBfC+DVhXkjSLhg2ULZPsDvwdcG6jfe8HrKmq66vqXuAc4IhxbY6gF2BVVRcBO3Z1DLOuJGkWDRso\n/wycB1xXVRcneThw7TT3vSdwY9/0Td28YdoMsy4ASVYkWZ1k9bp166ZZsiRpIkMFSlV9qqoeW1Wv\n6Kavr6rnzWxpbVTV6VW1vKqWL1y4cNTlSNIma9hvyi9K8rkkt3XDZ5Ismua+bwYW900v6uYN02aY\ndSVJs2jYU14fAVYCe3TDF7t503ExsDTJ3km2Bo7s9tFvJfCS7m6vA4BfVdXaIdeVJM2iqb7YOGZh\nVfUHyBlJXj2dHVfV+iTH0rs2swD4cFVdmeSYbvlpwCp639RfA9wDvGyydadTjyRpeoYNlNuTvAj4\neDd9FHD7dHdeVavohUb/vNP6xgt45bDrSpJGZ9hTXi+nd8vwrcBa4PnAS2eoJknSPDTsEco/A0eP\nPW6l+8b8O+gFjSRJQx+hPLb/2V1V9Qvg8TNTkiRpPho2ULboHgoJ/PEIZdijG0nSZmDYUHgn8J0k\nn+qmXwC8ZWZKkiTNR0MFSlWdlWQ18LRu1nOr6qqZK0uSNN8MfdqqCxBDRJI00LDXUCRJmpSBIklq\nwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiS\npCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEyMJlCQ7Jzk/ybXd\n350maHdIkmuSrElyfN/8k5L8KMkVST6XZMfZq16SNMiojlCOBy6sqqXAhd30/SRZAJwKHAosA45K\nsqxbfD7wmKp6LPBj4PWzUrUkaUKjCpQjgDO78TOB5wxosx+wpqqur6p7gXO69aiqr1TV+q7dRcCi\nGa5XkjSFUQXKblW1thu/FdhtQJs9gRv7pm/q5o33cuBLbcuTJG2oLWdqw0kuAB46YNGJ/RNVVUlq\nI/dxIrAeOHuSNiuAFQAPe9jDNmY3kqQhzFigVNXBEy1L8rMku1fV2iS7A7cNaHYzsLhvelE3b2wb\nLwUOBw6qqgkDqapOB04HWL58+UYFlyRpaqM65bUSOLobPxr4woA2FwNLk+ydZGvgyG49khwCHAc8\nu6rumYV6JUlTGFWgvBV4epJrgYO7aZLskWQVQHfR/VjgPOBq4JNVdWW3/inAdsD5SS5Lctpsd0CS\ndH8zdsprMlV1O3DQgPm3AIf1Ta8CVg1o9xczWqAkaYP5TXlJUhMGiiSpCQNFktSEgSJJasJAkSQ1\nYaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJ\nUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJA\nkSQ1YaBIkpowUCRJTYwkUJLsnOT8JNd2f3eaoN0hSa5JsibJ8QOWvy5JJdl15quWJE1mVEcoxwMX\nVtVS4MJu+n6SLABOBQ4FlgFHJVnWt3wx8Azgp7NSsSRpUqMKlCOAM7vxM4HnDGizH7Cmqq6vqnuB\nc7r1xrwLOA6omSxUkjScUQXKblW1thu/FdhtQJs9gRv7pm/q5pHkCODmqrp8qh0lWZFkdZLV69at\nm2bZkqSJbDlTG05yAfDQAYtO7J+oqkoy9FFGkm2AE+id7ppSVZ0OnA6wfPlyj2YkaYbMWKBU1cET\nLUvysyS7V9XaJLsDtw1odjOwuG96UTdvH2Bv4PIkY/MvTbJfVd3arAOSpA0yqlNeK4Gju/GjgS8M\naHMxsDTJ3km2Bo4EVlbVD6rqIVW1pKqW0DsVtq9hIkmjNapAeSvw9CTXAgd30yTZI8kqgKpaDxwL\nnAdcDXyyqq4cUb2SpCnM2CmvyVTV7cBBA+bfAhzWN70KWDXFtpa0rk+StOH8prwkqQkDRZLUhIEi\nSWrCQJEkNWGgSJKaMFAkSU0YKJKkJgwUSVITBookqQkDRZLUhIEiSWrCQJEkNWGgSJKaMFAkSU0Y\nKJKkJgwUSVITBookqQkDRZLUhIEiSWrCQJEkNWGgSJKaMFAkSU0YKJKkJgwUSVITqapR1zBrkqwD\nbhh1HRthV+Dnoy5iFm1u/QX7vLmYr33eq6oWTtVoswqU+SrJ6qpaPuo6Zsvm1l+wz5uLTb3PnvKS\nJDVhoEiSmjBQ5ofTR13ALNvc+gv2eXOxSffZayiSpCY8QpEkNWGgSJKaMFDmgCQ7Jzk/ybXd350m\naHdIkmuSrEly/IDlr0tSSXad+aqnZ7p9TnJSkh8luSLJ55LsOHvVb5ghXrckOblbfkWSfYddd67a\n2D4nWZzk35JcleTKJK+a/eo3znRe5275giTfT3Lu7FXdWFU5jHgA3g4c340fD7xtQJsFwHXAw4Gt\ngcuBZX3LFwPn0fvi5q6j7tNM9xl4BrBlN/62QevPhWGq161rcxjwJSDAAcB3h113Lg7T7PPuwL7d\n+HbAjzf1Pvctfy3wMeDcUfdnYwePUOaGI4Azu/EzgecMaLMfsKaqrq+qe4FzuvXGvAs4Dpgvd1lM\nq89V9ZWqWt+1uwhYNMP1bqypXje66bOq5yJgxyS7D7nuXLTRfa6qtVV1KUBV3QVcDew5m8VvpOm8\nziRZBDwT+OBsFt2agTI37FZVa7vxW4HdBrTZE7ixb/qmbh5JjgBurqrLZ7TKtqbV53FeTu+T31w0\nTB8majNs/+ea6fT5j5IsAR4PfLd5he1Nt8/vpveB8A8zVeBs2HLUBWwuklwAPHTAohP7J6qqkgx9\nlJFkG+AEeqeA5pSZ6vO4fZwIrAfO3pj1NTcl2Rb4DPDqqrpz1PXMpCSHA7dV1SVJDhx1PdNhoMyS\nqjp4omVJfjZ2uN8dAt82oNnN9K6TjFnUzdsH2Bu4PMnY/EuT7FdVtzbrwEaYwT6PbeOlwOHAQdWd\nhJ6DJu3DFG22GmLduWg6fSbJVvTC5Oyq+uwM1tnSdPr8PODZSQ4DHghsn+SjVfWiGax3Zoz6Io5D\nAZzE/S9Qv31Amy2B6+mFx9hFv0cPaPcT5sdF+Wn1GTgEuApYOOq+TNHPKV83eufO+y/Wfm9DXvO5\nNkyzzwHOAt496n7MVp/HtTmQeXxRfuQFOBTALsCFwLXABcDO3fw9gFV97Q6jd9fLdcCJE2xrvgTK\ntPoMrKF3Pvqybjht1H2apK9/1gfgGOCYbjzAqd3yHwDLN+Q1n4vDxvYZeAq9G0uu6HttDxt1f2b6\nde7bxrwOFB+9Iklqwru8JElNGCiSpCYMFElSEwaKJKkJA0WS1ISBok1Ckm93f5ck+YfG2z5h0L5m\nSpLnJHnjDG37hKlbbfA2/zLJGa23q/nH24a1SekeXfHfqurwDVhny/rTgyYHLb+7qrZtUd+Q9Xwb\neHZV/Xya2/mzfs1UX7rH7Ly8qn7aetuaPzxC0SYhyd3d6FuBpya5LMlrut+YOCnJxd1vUPxj1/7A\nJN9IspLeN+5J8vkkl3S/w7Gim/dW4EHd9s7u31f3+xYnJflhkh8k+fu+bX81yae732w5O91zcZK8\ntfutjyuSvGNAPx4B/G4sTJKckeS0JKuT/Lh77tPYb2cM1a++bQ/qy4uSfK+b9/4kC8b6mOQtSS5P\nclGS3br5L+j6e3mSr/dt/ovAkdN4CbUpGPU3Kx0cWgzA3d3fA+n7pjGwAnhDN/4AYDW9x2McCPwa\n2Luv7di39R8E/BDYpX/bA/b1POB8er+FsRvwU3q/53Eg8Ct6z2raAvgOvW+A7wJcw5/ODOw4oB8v\nA97ZN30G8OVuO0vpPaH2gRvSr0G1d+OPohcEW3XT7wVe0o0X8Kxu/O19+/oBsOf4+oEnA18c9X8H\nDqMdfDikNnXPAB6b5Pnd9A703pjvpfcspX/va/tfk/xtN764a3f7JNt+CvDxqroP+FmSrwFPBO7s\ntn0TQJLLgCX0frflt8CHul/lG/TLfLsD68bN+2RV/QG4Nsn1wCM3sF8TOQh4AnBxdwD1IP70kM57\n++q7BHh6N/4t4IwknwT6H9x4G73H5mgzZqBoUxfgv1TVefeb2bvW8utx0wcDT6qqe5J8ld6RwMb6\nXd/4ffR+XXJ9kv3ovZE/HzgWeNq49X5DLxz6jb/QWQzZrykEOLOqXj9g2e+ramy/99G9V1TVMUn2\np/egw0uSPKGqbqf3b/WbIferTZTXULSpuYveT8eOOQ94RfdIdJI8IsmDB6y3A3BHFyaPpPc02DG/\nH1t/nG8Af99dz1gI/BXwvYkK637jY4eqWgW8BnjcgGZXA38xbt4LkmyRZB96PzF7zQb0a7z+vlwI\nPD/JQ7pt7Jxkr8lWTrJPVX23qt5I70hq7HHsj6B3mlCbMY9QtKm5ArgvyeX0rj+8h97ppku7C+Pr\nGPxzw18GjklyNb037Iv6lp0OXJHk0qp6Yd/8zwFPoveo8gKOq6pbu0AaZDvgC0keSO/o4LUD2nwd\neGeS9B0h/JReUG1P78m1v03ywSH7Nd79+pLkDcBXkmwB/B54JXDDJOuflGRpV/+FXd8B/gb4v0Ps\nX5swbxuW5pgk76F3gfuC7vsd51bVp0dc1oSSPAD4GvCUmuT2a236POUlzT3/C9hm1EVsgIfR+7E0\nw2Qz5xGKJKkJj1AkSU0YKJKkJgwUSVITBookqQkDRZLUxP8HKpB65fIpweQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1bb295940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "Train Accuracy: 0.940741\n",
      "Test Accuracy: 0.783333\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**: although it may not match perfectly, your expected output should be close to ours and your cost value should decrease.\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "    <td> \n",
    "    **Cost after epoch 0 =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      1.917929\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Cost after epoch 5 =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      1.506757\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Train Accuracy   =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.940741\n",
    "    </td> \n",
    "</tr> \n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **Test Accuracy   =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.783333\n",
    "    </td> \n",
    "</tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have finised the assignment and built a model that recognizes SIGN language with almost 80% accuracy on the test set. If you wish, feel free to play around with this dataset further. You can actually improve its accuracy by spending more time tuning the hyperparameters, or using regularization (as this model clearly has a high variance). \n",
    "\n",
    "Once again, here's a thumbs up for your work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc170282da0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc15263b400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = \"images/thumbs_up.jpg\"\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(64,64))\n",
    "plt.imshow(my_image)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
